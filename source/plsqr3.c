/*

-dir: data directory path, data_path
-ker_i: ker_col_info
-ker_f: ker_col_bin_filename
-damp_i: damp_row_Info
-damp_f: dampingFileName, binary or ascii
-col_perm: column permutation, col_perm, deprecated
-b_k: vector kernel part, vectorB_k
-row_k: numRow_k
-row_d: numRow_d
-col: numCol_global
-itn: iteration, itn


==============================================

*/

#include "plsqr3.h"
#include "vecops.h"
#include <assert.h>

//#define DEBUG
//#define DEBUG_GDB

/*enable this if damping is using binary file,
 * disable this if using ASCII damping file */
#define DAMP_BIN


/*matrix Index types:
global index: matrix=kernel+damping
global index in kernel: matrix=kernel
global index in damping: matrix=kernel
local index in block: the block can be either kernel matrix or damping matrix
 *
 * */
/*** group_size_omp (number of chunks) from all omp directive. group_size_omp=number of threads * 10 */
int group_size_omp=1;
int PLSQR3_main( int argc, char * argv[], int procRank, int numProc)
{
	int nthreads, tid;
	/* Fork a team of threads giving them their own copies of variables */
#ifdef _OPENMP
#pragma omp parallel private(nthreads, tid)
	{	/* Obtain thread number */
		tid = omp_get_thread_num();
		printf("OMP tid = %d on MPI rank = %d\n", tid, procRank);

		/* Only master thread does this */
		if (procRank==0 && tid == 0 ) {
			nthreads = omp_get_num_threads();
			printf("OpenMP info: Number of threads = %d in MPI rank = %d \n", nthreads, procRank);
			group_size_omp=nthreads * 10;
		}
	} /* All threads join master thread and disband */
#endif
	int i = 0;
	char idstr[32];
	char buff[BUFSIZE];
/*	MPI_Status status;*/
	if(procRank==0){
		printf("------------------------PLSQR3---------------------------\n\n\n");
#ifdef TRIDIAGONAL
		printf("------------------------Multi-diagonal----------------------\n");
#endif


#ifdef DAMP_BIN
		printf("------------------------Binary damping file ------------------------\n");
#else
		printf("------------------------ASCII damping file------------------------\n");
#endif
#ifdef COL_PERM
		fprintf(stdout, "%s(%d)-%s: ------------------------use COLUMN PERMUTATION------------------------\n", __FILE__,__LINE__,__FUNCTION__);
#else
		fprintf(stdout, "%s(%d)-%s: ------------------------NO column permutation------------------------\n", __FILE__,__LINE__,__FUNCTION__);
#endif

#if MPICOMMTYPE==1
		fprintf(stdout, "%s(%d)-%s: ------------------One Sided Communication---------------\n", __FILE__,__LINE__,__FUNCTION__);
#elif MPICOMMTYPE==2
		fprintf(stdout, "%s(%d)-%s: ------------------Point to Point Communication---------------\n", __FILE__,__LINE__,__FUNCTION__);
#endif

#if GPUALL ==1
	printf("------------------------PLSQR3 GPU version---------------------------\n");
#else
	printf("------------------------PLSQR3 CPU version---------------------------\n");
#endif

#ifdef USE_COMPRESSED_VEC
	printf("------------------------Use Compressed Vector---------------------------\n");
#endif
	}

#if GPUALL ==1
	initCUDADevice(procRank);
#endif

//////////////////////////////////////////////////////////////////////


	char data_path[MAXREADNUM]="../data_plsqr3";
	char ker_col_info[MAXREADNUM] = "kernel_col.mm2.info";
	char ker_col_bin_filename[MAXREADNUM] = "kernel_col.mm2.bin";
	char damp_row_Info[MAXREADNUM]="plsqr3_reorder_small_all.damp.info.bin";
	char dampingFileName[MAXREADNUM]="plsqr3_reorder_small_all.damp.bin"/*"reordered_matrix_small.damp"*/;
	char damp_col_Info[MAXREADNUM]="plsqr3_reorder_small_all.damp.bycol.info.bin";
	char damping_bycolFileName[MAXREADNUM]="plsqr3_reorder_small_all.damp.bycol.bin";
	char col_perm[MAXREADNUM]="colPerm_fake";
	char vectorB_k[MAXREADNUM]="vecotrB_ker"; //the damping part is all zero and is generated by the program;
	char row_partition[MAXREADNUM]="";
	char col_partition[MAXREADNUM]="";
	int numRow_k=10;
	int numRow_d=1825;
	int numCol_global = 520;
	int itn = 40 ;



	for(i=1; i<argc; i++){
		if(strcmp(argv[i], "-dir")==0){
			i++;
			strcpy(data_path, argv[i]);
		}
		else if(strcmp(argv[i], "-ker_i")==0){
			i++;
			strcpy(ker_col_info, argv[i]);
		}
		else if(strcmp(argv[i], "-ker_f")==0){
			i++;
			strcpy(ker_col_bin_filename, argv[i]);
		}
		else if(strcmp(argv[i], "-damp_i")==0){
			i++;
			strcpy(damp_row_Info, argv[i]);
		}
		else if(strcmp(argv[i], "-damp_f")==0){
			i++;
			strcpy(dampingFileName, argv[i]);
		}
		else if(strcmp(argv[i], "-damp_i_bycol")==0){
			i++;
			strcpy(damp_col_Info, argv[i]);
		}
		else if (strcmp(argv[i], "-damp_f_bycol")==0){
			i++;
			strcpy(damping_bycolFileName, argv[i]);
		}
		else if(strcmp(argv[i], "-col_perm")==0){
			i++;
			strcpy(col_perm, argv[i]);
		}
		else if(strcmp(argv[i], "-b_k")==0){
			i++;
			strcpy(vectorB_k, argv[i]);
		}
		else if(strcmp(argv[i], "-row_k")==0){
			i++;
			numRow_k=atoi(argv[i]);
		}
		else if(strcmp(argv[i], "-row_d")==0){
			i++;
			numRow_d=atoi(argv[i]);
		}
		else if(strcmp(argv[i], "-col")==0){
			i++;
			numCol_global=atoi(argv[i]);
		}
		else if(strcmp(argv[i], "-itn")==0){
			i++;
			itn=atoi(argv[i]);
		}
		else if(strcmp(argv[i], "-row_ptn")==0){
			i++;
			strcpy(row_partition, argv[i]);
		}else if(strcmp(argv[i], "-col_ptn")==0){
			i++;
			strcpy(col_partition, argv[i]);
		}
		else{
			fprintf(stderr, "%d: cannot recognize %s\n", procRank, argv[i]);
		}
	}

//----------------------------------------------------------------------------------------------------------------------------------------------------------

	//full path
	char ker_col_info_f[MAXREADNUM],ker_col_bin_f[MAXREADNUM],
	damp_row_Info_f[MAXREADNUM], dampingFileName_f[MAXREADNUM],
	damp_col_Info_f[MAXREADNUM], damping_bycolFileName_f[MAXREADNUM],
	col_perm_f[MAXREADNUM],vectorB_k_f[MAXREADNUM],
	row_partition_f[MAXREADNUM]="", col_partition_f[MAXREADNUM]="";

	sprintf(ker_col_info_f, "%s/%s", data_path, ker_col_info);
	sprintf(ker_col_bin_f, "%s/%s",data_path,  ker_col_bin_filename);
	sprintf(damp_row_Info_f, "%s/%s",data_path, damp_row_Info);
	sprintf(dampingFileName_f, "%s/%s",data_path,  dampingFileName);
	sprintf(damp_col_Info_f, "%s/%s", data_path, damp_col_Info);
	sprintf(damping_bycolFileName_f, "%s/%s", data_path, damping_bycolFileName);
	sprintf(col_perm_f, "%s/%s",data_path, col_perm);
	sprintf(vectorB_k_f, "%s/%s",data_path, vectorB_k);
	if(strlen(row_partition)!=0) sprintf(row_partition_f, "%s/%s", data_path, row_partition);
	if(strlen(col_partition)!=0) sprintf(col_partition_f, "%s/%s", data_path, col_partition);
	if(procRank==0){
		printf("ker_col_info_f = %s\nker_col_bin_f = %s\ndamp_row_Info_f=%s\ndampingFileName_f = %s\ncol_perm_f = %s\nvectorB_k_f = %s\n damp_col_Info_f=%s\n damping_bycolFileName_f=%s\n",
				ker_col_info_f, ker_col_bin_f, damp_row_Info_f, dampingFileName_f,col_perm_f,vectorB_k_f, damp_col_Info_f, damping_bycolFileName_f);
		if(strlen(row_partition_f)!=0 && strlen(col_partition_f)!=0){
			printf("row_partition_f= %s \ncol_partition_f=%s\n", row_partition_f, col_partition_f);}
		else
			printf("row_partition_f=%d col_partition_f=%d\n", strlen(row_partition_f), strlen(col_partition_f));
	}

	int numRow_global = numRow_k + numRow_d;

	//Kernel IO (((--------------------------------------------------------------------------------------------------------------------------------------
		KernelColInfoPerCol * kernelColInfoArray = (KernelColInfoPerCol *) calloc(numCol_global,sizeof(KernelColInfoPerCol));
		if (kernelColInfoArray == NULL) {
			printf("ERROR ALLOCATING kernelInfo\n");			exit(-1);
		}

		read_kernel_col_info(procRank, ker_col_info_f/*"kernel_col.mm2.info"*/, numCol_global, kernelColInfoArray);

	//load balance kernel component((((		//average number of column allocated to per process
		int ave_numCol_k=0, remainder_k=0, numColPerProc_k=0;
		ave_numCol_k=numCol_global / numProc;
		remainder_k= numCol_global % numProc;
		if(remainder_k==0){
			numColPerProc_k=ave_numCol_k;
		}else { // 0<remainder<numProc
			if(procRank<remainder_k){
				numColPerProc_k=ave_numCol_k+1;
			}else{
				numColPerProc_k=ave_numCol_k;
			}
		}
		int startCol_perProc_k=0, endCol_perProc_k =0;
		if(procRank<remainder_k){
			startCol_perProc_k=procRank*(ave_numCol_k+1);
		}else{
			startCol_perProc_k=procRank*ave_numCol_k+remainder_k;
		}
		endCol_perProc_k=startCol_perProc_k+numColPerProc_k-1;

		if(strlen(col_partition_f)!=0){
			readPartitionFile(col_partition_f, procRank, numProc, &startCol_perProc_k, &endCol_perProc_k);
			numColPerProc_k = endCol_perProc_k - startCol_perProc_k + 1 ;
		}

#ifdef DEBUG
		fprintf(stdout, "%d: startCol=%d, endCol=%d, numCol=%d\n", procRank, startCol_perProc_k, endCol_perProc_k, numColPerProc_k);
#endif
		//load balance kernel component))))

		long long nnzPerProc_k=0;
		KernelColInfoPerCol * kernelColInfo_perProc=(KernelColInfoPerCol *)malloc(sizeof(KernelColInfoPerCol)*numColPerProc_k);

		//column permutation
#ifdef COL_PERM
		int * colPerm_array = (int *) calloc(numCol_global, sizeof(int));
		if (colPerm_array == NULL) {
			fprintf(stderr, "ERROR ALLOCATING colPerm_all");			exit(-1);
		}		//read column permutation
		read_col_perm(col_perm_f/*"ColPerm"*/, colPerm_array, numCol_global);
#endif
		//covert global kerColInfo to localKerColInfo, p1 is the local pointer, p2 is the global pointer
		for (i = startCol_perProc_k; i <= endCol_perProc_k; i++) {
			KernelColInfoPerCol * p1 = &kernelColInfo_perProc[i - startCol_perProc_k];
#ifdef COL_PERM
			KernelColInfoPerCol * p2 = &kernelColInfoArray[colPerm_array[i]];
			nnzPerProc_k += p2->nnz;
#else
			KernelColInfoPerCol * p2 = &kernelColInfoArray[i];
			nnzPerProc_k += p2->nnz;//
#endif
			//p1<=p2
			p1->colIdx=p2->colIdx;
			p1->globalBeginIdx=p2->globalBeginIdx;
			p1->nnz= p2->nnz;
		}
		free(kernelColInfoArray);kernelColInfoArray=NULL;

#ifdef COL_PERM
		//	scatter all process's column permutation information to individual process
		sendcnts=(int * )calloc(numProc, sizeof(int));
		if(NULL==sendcnts){
			fprintf(stderr, "sendcnts calloc fails \n");exit(-1);
		}
		displs=(int * )calloc(numProc, sizeof(int));
		if(NULL==displs){
			fprintf(stderr, "displs calloc fails \n");exit(-1);
		}
		MPI_Allgather(&numColPerProc_k, 1, MPI_INT, sendcnts, 1, MPI_INT, MPI_COMM_WORLD);
		displs[0]=0;
		for(i=1; i<numProc; i++){
			displs[i]=displs[i-1]+sendcnts[i-1];
		}
		int * colPerm_perProc;
		colPerm_perProc= (int *) calloc(numColPerProc_k, sizeof(int));
		if (colPerm_perProc == NULL) {
			fprintf(stderr, "ERROR ALLOCATING colPerm_perProc");			exit(-1);
		}
		MPI_Scatterv(colPerm_array, sendcnts, displs, MPI_INT,
				colPerm_perProc, numColPerProc_k, MPI_INT, 0, MPI_COMM_WORLD);

		free(sendcnts);
		free(displs);
#endif

	#ifdef DEBUG
		MPI_Barrier(MPI_COMM_WORLD);
	#endif
	//))

	//load kernel(( the rowIdx here belongs to 3 types: global index: matrix=kernel+damping;
	//	global index in kernel: matrix=kernel; and local index in block: the block is  kernel matrix
	if(procRank==0)
		printf("%d: %s(%d)-%s: Parallel loading kernel (binary)...\n", procRank, __FILE__,__LINE__,__FUNCTION__);
	//kernel matrix: local column index, global row index (equivalent to local index)
	SpMatCSC spMatCSC_perProc_k;
	initSpMatCSC(&spMatCSC_perProc_k, numRow_k, numColPerProc_k, nnzPerProc_k);
#ifdef GPTL
	GPTLstart("loadKernel");
#endif
#ifdef COL_PERM
	mpiio_load_kernelColBin2ReorderedCSC(procRank, numProc, ker_col_bin_f, numCol_global,
				kernelColInfoArray, colPerm_perProc, &spMatCSC_perProc_k);
	free(colPerm_perProc);
#else
/*	mpiio_load_kernelColBin2CSC_multi(procRank, numProc,	ker_col_bin_f,
			nnzPerProc_k,  kernelColInfo_perProc, &spMatCSC_perProc_k);*/
	mpiio_load_kernelColBin2CSC_once(procRank, numProc,	ker_col_bin_f,
			nnzPerProc_k,  kernelColInfo_perProc, &spMatCSC_perProc_k);
#endif
	MPI_Barrier(MPI_COMM_WORLD);
#ifdef GPTL
	GPTLstop("loadKernel");//load kernel))
#endif
	free(kernelColInfo_perProc);
	if(procRank==0){
		fprintf(stdout, "%d: %s(%d)-%s: Parallel loading kernel (binary) DONE!\n", procRank, __FILE__,__LINE__,__FUNCTION__);
	}
	//Kernel IO )))------------------------------------------------------------------------------------------------------------------------


	//damping matrix processing((
	//damping matrix total number of row, total number of nonzeros

	//load balance : damping <<
	int ave_numCol_d=0, remainder_d=0, numRow_perProc_d=0, startRowIdx_perProc_d=0, endRowIdx_perProc_d=0;
	ave_numCol_d=numRow_d/numProc;
	remainder_d=numRow_d%numProc;
	if(remainder_d==0){
		numRow_perProc_d=ave_numCol_d;
	}else{
		if(procRank<remainder_d){
			numRow_perProc_d=ave_numCol_d+1;
		}else{
			numRow_perProc_d=ave_numCol_d;
		}
	}
	if(procRank<remainder_d){
		startRowIdx_perProc_d=procRank*(ave_numCol_d+1);
	}else{
		startRowIdx_perProc_d=procRank*ave_numCol_d+remainder_d;
	}
	endRowIdx_perProc_d=startRowIdx_perProc_d+numRow_perProc_d-1 ;

	if(strlen(row_partition_f)!=0){
		readPartitionFile(row_partition_f, procRank, numProc, &startRowIdx_perProc_d, &endRowIdx_perProc_d);
		numRow_perProc_d = endRowIdx_perProc_d - startRowIdx_perProc_d + 1 ;
	}
#ifdef DEBUG
	printf("%d: startRowIdx_perProc_d=%d endRowIdx_perProc_d=%d num of row=%d",procRank, startRowIdx_perProc_d, endRowIdx_perProc_d, numRow_perProc_d);
#endif
	//load balance : damping >>

//-------------------------load damping infomation------------------------------------------------------------<<
	int * nnzPerRow_perProc_d_array = (int *)calloc(numRow_perProc_d, sizeof(int));
	if(nnzPerRow_perProc_d_array==NULL){
		fprintf(stderr, "%d: %s(%d)-%s: can not allocate \n", procRank , __FILE__,__LINE__,__FUNCTION__);
		exit(EXIT_FAILURE);
	}
	memset(nnzPerRow_perProc_d_array, 0, numRow_perProc_d*sizeof(int));

#ifdef DAMP_BIN
	//read using MPI IO; mpiio_load_damp_info(procRank, numProc,
	mpiio_load_damp_info( procRank, numProc, damp_row_Info_f, startRowIdx_perProc_d, numRow_perProc_d, nnzPerRow_perProc_d_array);
#else
	if(procRank==0)
		printf("%d: Scanning (serial)damping info file (ascii)...\n", procRank);
	countDampingData_ascii(dampingFileName_f, numRow_d, nnzPerRow_d_array, &all_nnz_d);
#endif


#ifndef DAMP_BIN
	if(procRank==0)
		printf("%d: Loading (serial) damping file (ascii)...\n", procRank);
	BasicSpMat2 basicSpMat2_all_d;
	basicSpMat2_all_d.numCol=0;
	basicSpMat2_all_d.numRow=0;
	basicSpMat2_all_d.numNonzero=0;
	basicSpMat2_all_d.count=0;
	if(procRank==0){
		init_BasicSpMat2(& basicSpMat2_all_d, numRow_d,  numCol_global, all_nnz_d);
		loadDampingData_ascii(dampingFileName_f, &basicSpMat2_all_d);
	}
#endif

	long long nnz_perProc_d=0;

	int chunk=(numRow_perProc_d + group_size_omp - 1) / (group_size_omp);
#pragma omp parallel for private(i) schedule(dynamic, chunk) reduction(+:nnz_perProc_d)
	for(i=0; i<numRow_perProc_d; i++ ){
		nnz_perProc_d+=nnzPerRow_perProc_d_array[i];
	}
	free(nnzPerRow_perProc_d_array);nnzPerRow_perProc_d_array=NULL;


#ifdef DEBUG
	fprintf(stdout, "%d: numRow_perProc_d=%d\n", procRank, numRow_perProc_d);
	fprintf(stdout, "%d: numNonzero_perProc_d=%d\n", procRank, nnz_perProc_d);
#endif

//------------------------------------------------------------------------------------------------------->>


/* Damping IO
 * damping matrix: local row index, global column index
 * (converted to local index in initPerProcDataV3()in One-diagonal case)
*/
	SpMatCSR spMatCSR_perProc_d;
#ifdef DAMP_BIN
	// init spMatCSR_perProc_d
	initSpMatCSR(&spMatCSR_perProc_d, numRow_perProc_d, numCol_global, nnz_perProc_d);
	//MPIIO load damping
	if(procRank==0)
		printf("%d: Loading (parallel) damping file (Binary)...\n", procRank);
#ifdef GPTL
	GPTLstart("loadDamp");
#endif
	mpiio_load_damp_once(procRank, numProc, dampingFileName_f, nnz_perProc_d, &spMatCSR_perProc_d, CSR);
#ifdef GPTL
	GPTLstop("loadDamp");
#endif
#else
	int * sendcnts,  * displs;
	/*basicSpMat2_perProc_d's row and column index are all global index in damping: matrix=kernel*/
	BasicSpMat2 basicSpMat2_perProc_d;
	init_BasicSpMat2(&basicSpMat2_perProc_d, numRow_perProc_d, numCol_global, nnz_perProc_d);
//scatter the basicSpMat_all_d((((
	sendcnts=(int * )calloc(numProc, sizeof(int));
	if(NULL==sendcnts){
		fprintf(stderr, "sendcnts calloc fails \n");exit(-1);
	}
	displs=(int * )calloc(numProc, sizeof(int));
	if(NULL==displs){
		fprintf(stderr, "displs calloc fails \n");exit(-1);
	}
	MPI_Allgather(&basicSpMat2_perProc_d.numNonzero, 1, MPI_INT, sendcnts, 1, MPI_INT, MPI_COMM_WORLD);
	displs[0]=0;
	for(i=1; i<numProc; i++){
		displs[i]=displs[i-1]+sendcnts[i-1];
	}
	//rowIdx is the global index, convert to local index later
	MPI_Scatterv(basicSpMat2_all_d.rowIdx, sendcnts, displs, MPI_INT,
			basicSpMat2_perProc_d.rowIdx, basicSpMat2_perProc_d.numNonzero, MPI_INT, 0, MPI_COMM_WORLD);
	MPI_Scatterv(basicSpMat2_all_d.colIdx, sendcnts, displs, MPI_INT,
			basicSpMat2_perProc_d.colIdx, basicSpMat2_perProc_d.numNonzero, MPI_INT, 0, MPI_COMM_WORLD);
#if SDVERSION==1
	MPI_Scatterv(basicSpMat2_all_d.val, sendcnts, displs, MPI_FLOAT,
			basicSpMat2_perProc_d.val, basicSpMat2_perProc_d.numNonzero, MPI_FLOAT, 0, MPI_COMM_WORLD);
#elif SDVERSION == 2
	MPI_Scatterv(basicSpMat2_all_d.val, sendcnts, displs, MPI_DOUBLE,
			basicSpMat2_perProc_d.val, basicSpMat2_perProc_d.numNonzero, MPI_DOUBLE, 0, MPI_COMM_WORLD);
#endif
	free(sendcnts);
	free(displs);
	if(procRank==0){
		finalizeBasicSpMat2(& basicSpMat2_all_d);
	}
//scatter the basicSpMat_all_d))))

	/*spMatCSR_perProc_d colIdx is global index in damping: matrix=kernel, also global index: matrix=kernel+damping*/

	init_SparseMatrixCSR_fromBasicSpMat2(&spMatCSR_perProc_d, &basicSpMat2_perProc_d);
	basicSpMat2TOSpMatCSR(& spMatCSR_perProc_d, &basicSpMat2_perProc_d, procRank);
	finalizeBasicSpMat2(&basicSpMat2_perProc_d);
#endif


//load damping matrix by column
#ifdef 	 TRIDIAGONAL
	int * nnzPerCol_d_array=(int *)malloc(numCol_global*sizeof(int));
	if(nnzPerCol_d_array==NULL){
		fprintf(stderr, "%s(%d)-%s: \n", __FILE__,__LINE__,__FUNCTION__);exit(EXIT_FAILURE);
	}
	memset(nnzPerCol_d_array, 0, numCol_global*sizeof(int));
	if(procRank==0)
		printf("%d: Scanning (serial)damping by column file (binary)...\n", procRank);
	FILE * f_damp_bycol_info=fopen(damp_col_Info_f, "rb");
	if(f_damp_bycol_info==NULL){
		fprintf(stderr, "%s(%d)-%s: f_damp_bycol_info==NULL\n", __FILE__,__LINE__,__FUNCTION__);exit(EXIT_FAILURE);
	}
	fread(nnzPerCol_d_array, sizeof(int), numCol_global, f_damp_bycol_info);
	fclose(f_damp_bycol_info);

	long long nnz_perProc_d_bycol=0;

	chunk=((endCol_perProc_k-startCol_perProc_k+1) + group_size_omp - 1) / (group_size_omp);
#pragma omp parallel for private(i)  schedule(dynamic, chunk) reduction(+:nnz_perProc_d_bycol)
	for(i=startCol_perProc_k; i<=endCol_perProc_k; i++)
	{
		nnz_perProc_d_bycol+=nnzPerCol_d_array[i];
	}
	free(nnzPerCol_d_array);

	SpMatCSC spMatCSC_d_bycol_perProc;
	initSpMatCSC(&spMatCSC_d_bycol_perProc, numRow_d, numColPerProc_k, nnz_perProc_d_bycol);
	//MPIIO load damping
	if(procRank==0)
		printf("%d: Loading (parallel) damping by column file (Binary)...\n", procRank);
#ifdef GPTL
	GPTLstart("loadDamp_bycol");
#endif
	mpiio_load_damp_once(procRank, numProc, damping_bycolFileName_f, nnz_perProc_d_bycol, &spMatCSC_d_bycol_perProc, CSC);
#ifdef GPTL
	GPTLstop("loadDamp_bycol");
#endif
#endif


//check load balance info <<
	int min_nnz_pp_d, max_nnz_pp_d, sum_nnz_pp_d, ave_nnz_pp_d, 
            min_nnz_pp_k, max_nnz_pp_k, sum_nnz_pp_k, ave_nnz_pp_k;

	MPI_Reduce(&spMatCSC_perProc_k.numNonzero, &min_nnz_pp_k, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);
	MPI_Reduce(&spMatCSC_perProc_k.numNonzero, &max_nnz_pp_k, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);
	MPI_Reduce(&spMatCSC_perProc_k.numNonzero, &sum_nnz_pp_k, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

	MPI_Reduce(&spMatCSR_perProc_d.numNonzero, &min_nnz_pp_d, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);
	MPI_Reduce(&spMatCSR_perProc_d.numNonzero, &max_nnz_pp_d, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);
	MPI_Reduce(&spMatCSR_perProc_d.numNonzero, &sum_nnz_pp_d, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
	if(procRank==0){
		ave_nnz_pp_d=sum_nnz_pp_d/numProc;
		printf("%d: min number of nonzero =%d in damping among all processes\n", procRank, min_nnz_pp_d);
		printf("%d: max number of nonzero =%d in damping among all processes\n", procRank, max_nnz_pp_d);
		printf("%d: ave number of nonzero =%d in damping among all processes\n", procRank, ave_nnz_pp_d);
		ave_nnz_pp_k=sum_nnz_pp_k/numProc;
		printf("%d: min number of nonzero =%d in kernel among all processes\n", procRank, min_nnz_pp_k);
		printf("%d: max number of nonzero =%d in kernel among all processes\n", procRank, max_nnz_pp_k);
		printf("%d: ave number of nonzero =%d in kernel among all processes\n", procRank, ave_nnz_pp_k);
	}

	MPI_Reduce(&spMatCSC_d_bycol_perProc.numNonzero, &min_nnz_pp_d, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);
	MPI_Reduce(&spMatCSC_d_bycol_perProc.numNonzero, &max_nnz_pp_d, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);
	MPI_Reduce(&spMatCSC_d_bycol_perProc.numNonzero, &sum_nnz_pp_d, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
	if(procRank==0){
		ave_nnz_pp_d=sum_nnz_pp_d/numProc;
		printf("%d: min number of nonzero =%d in damping by column among all processes\n", procRank, min_nnz_pp_d);
		printf("%d: max number of nonzero =%d in damping by column among all processes\n", procRank, max_nnz_pp_d);
		printf("%d: ave number of nonzero =%d in damping by column among all processes\n", procRank, ave_nnz_pp_d);
	}
//check load balance info >>

	//construct data structure for every process
/*	Initialization STEP 1/2:
 * Before this step: 12/8/2011
 * kernel matrix (CSC) in each MPI task has global row index and local column index
 * 	damping matrix (CSR) in each MPI task has local row index and global column index
 * */
	PPDataV3_TRI perProcData;
	initPPDataV3_TRI( procRank, numProc, & perProcData,&spMatCSC_perProc_k ,&spMatCSR_perProc_d , &spMatCSC_d_bycol_perProc,
		 numRow_k,  startCol_perProc_k,  endCol_perProc_k, startRowIdx_perProc_d, endRowIdx_perProc_d);

//Initialization STEP 2/2: initialize vec y     initialize vector y's kernel part, i.e. vector b, USE 1 TEMPERIALY,
//	use loadVecB to replace in real application
	FILE * vec_fp=fopen(vectorB_k_f, "r");
	if(vec_fp==NULL){fprintf(stderr, "cannot open %s\n",vectorB_k_f); exit(-1);}
	for(i=0; i<perProcData.matCSC_k->numRow; i++){
#if SDVERSION==1
		fscanf(vec_fp, "%e", &(perProcData.vec_y_k[i]));
#elif SDVERSION == 2
		fscanf(vec_fp, "%le", &(perProcData.vec_y_k[i]));
#endif
	}
	fclose(vec_fp);
//End of Initialization STEP 2/2

//process vector v, w, x <<
	MY_FLOAT_TYPE * v = perProcData.vec_x;// v is vector x
	MY_FLOAT_TYPE* w = (MY_FLOAT_TYPE*) calloc(perProcData.vec_x_len, sizeof(MY_FLOAT_TYPE));
	MY_FLOAT_TYPE* x = (MY_FLOAT_TYPE*) calloc(perProcData.vec_x_len, sizeof(MY_FLOAT_TYPE));
	if(NULL==w){
		fprintf(stderr, "w calloc fails \n");exit(-1);
	}
	if(NULL==x){
		fprintf(stderr, "x calloc fails \n");exit(-1);
	}
	memset(w, 0, perProcData.vec_x_len* sizeof(MY_FLOAT_TYPE));
	memset(x, 0, perProcData.vec_x_len* sizeof(MY_FLOAT_TYPE));
//process vector v, w, x >>


#ifdef COL_PERM
	call_lsqr(perProcData.version, procRank, numProc, numRow_global, numCol_global, &perProcData,
		NULL,v , w, x, itn, colPerm_array);
	free(colPerm_array);

#else
	call_lsqr(perProcData.version, procRank, numProc, numRow_global, numCol_global, &perProcData,
		NULL,v , w, x, itn, NULL/*colPerm_array*/);
#endif

	/*Free Memory*/
	finalPPDataV3_TRI(& perProcData);
	free(w);
	free(x);
	finalizeSpMatCSC(&spMatCSC_perProc_k);
	finalizeSpMatCSR(&spMatCSR_perProc_d);
	return 0;
}







/*9/1/2011*/

void initPerProcDataV3(int procRank, int numProc, PerProcDataV3 * p, SpMatCSC * block_k, SpMatCSR * block_d,
		int numRow_k, int startCol_perProc_k, int endCol_perProc_k,
		int startRowIdx_perProc_d, int numRow_perProc_d)
{
	p->version=PLSQRV3;
	p->subversion=PLSQRV3ONE;
	int i;
//kernel matrix
	p->matCSC_k=block_k;
	//Boundary of kernel block
	p->ai_k=0;
	p->aj_k=startCol_perProc_k;
	p->ci_k=numRow_k-1;
	p->cj_k=endCol_perProc_k;

//damping matrix, now damping matrix 's row index is local, but column index is global
	p->matCSR_d=block_d;
	p->ai_d=numRow_k + startRowIdx_perProc_d;
	//Boundary of damping block
	int aj_d_index;// index of p->aj_d
	min_vec1(block_d->col_idx, block_d->numNonzero, &aj_d_index, &p->aj_d);
	p->ci_d=numRow_k + startRowIdx_perProc_d + numRow_perProc_d-1;
	int cj_d_index; //index of p->cj_d
	max_vec1(block_d->col_idx, block_d->numNonzero, &cj_d_index, &p->cj_d);

/*	printf("%d: %s(%d)-%s:) p->aj_d[%d]=%d p->ci_d[%d]=%d\n", procRank, __FILE__,__LINE__,__FUNCTION__,
			aj_d_index, p->aj_d, cj_d_index, p->ci_d);*/

//9/1/2011 check whether local process has overlap with its next next neighbors; use column global index <<<
	int * aj_d_allproc = (int *) malloc(numProc * sizeof(int));
	int * cj_d_allproc = (int *) malloc(numProc * sizeof(int));
	memset(aj_d_allproc, 0, numProc * sizeof(int));
	memset(cj_d_allproc, 0, numProc * sizeof(int));
	MPI_Allgather(&p->aj_d, 1, MPI_INT, aj_d_allproc, 1, MPI_INT, MPI_COMM_WORLD);
	MPI_Allgather(&p->cj_d, 1, MPI_INT, cj_d_allproc, 1, MPI_INT, MPI_COMM_WORLD);
	if(p->cj_d >= p->aj_d){}
	else {
		fprintf(stderr, "%s(%d)-%s: rank=%d: p->cj_d=%d p->aj_d=%d\n",
				__FILE__,__LINE__,__FUNCTION__, procRank, p->cj_d, p->aj_d);
		exit(-1);
	}
	//check overlap condition
	if(procRank+1<numProc){//	aj_d(k+1)<= cj_d(k) <= cj_d(k+1)
/*		if(p->cj_d >= aj_d_allproc[procRank+1]){}
		else{
			fprintf(stderr, "%s(%d)-%s: rank=%d: p->cj_d=%d aj_d_allproc[procRank+1]=%d\n",
					__FILE__,__LINE__,__FUNCTION__, procRank, p->cj_d , aj_d_allproc[procRank+1]);
			fprintf(stderr, "please change number of process\n");
			exit(-1);
		}*/
		if(p->cj_d <= cj_d_allproc[procRank+1]){}
		else{
			fprintf(stderr, "%s(%d)-%s: rank=%d: p->cj_d=%d > cj_d_allproc[procRank+1]=%d\n",
					__FILE__,__LINE__,__FUNCTION__, procRank, p->cj_d, cj_d_allproc[procRank+1] );
			exit(-1);
		}
	}
	//check overlap condition
	if(procRank+2<numProc){//cj_d(k) < aj(k+2)
		if(p->cj_d < aj_d_allproc[procRank+2]){}
		else{
			fprintf(stderr, "WARNING! %s(%d)-%s: rank=%d: p->cj_d=%d >= j_d_allproc[procRank+2]=%d \t Additional info: a[%d,%d], c[%d, %d]\n",
					__FILE__,__LINE__,__FUNCTION__, procRank, p->cj_d,  aj_d_allproc[procRank+2],
					p->ai_d, p->aj_d, p->ci_d, p->cj_d );
			exit(-1);
		}
	}

	free(aj_d_allproc);
	free(cj_d_allproc);
//end of check >>>

//convert damping matrix column index into local index
	for(i=0; i<p->matCSR_d->numNonzero; i++){
		p->matCSR_d->col_idx[i] -= p->aj_d;
	}


//vector y, allocate memory, decomposed vector
	p->vec_y_begin_gloIdx_d=p->ai_d;
	p->vec_y_end_gloIdx_d=p->ci_d;
	p->vec_y_k=(MY_FLOAT_TYPE *)calloc(numRow_k, sizeof(MY_FLOAT_TYPE));
	memset(p->vec_y_k, 0, numRow_k*sizeof(MY_FLOAT_TYPE));
	if(p->vec_y_k==NULL){fprintf(stderr, "allocating p->vec_y_k fails\n");}
	int len1=p->vec_y_end_gloIdx_d - p->vec_y_begin_gloIdx_d + 1;
	p->vec_y_d=(MY_FLOAT_TYPE *)calloc(len1,sizeof(MY_FLOAT_TYPE));
	if(p->vec_y_d==NULL){fprintf(stderr, "allocating p-> vec_y_d fails\n");}
	memset(p->vec_y_d, 0, len1*sizeof(MY_FLOAT_TYPE));

//vector x, allocate memory, decomposed vector
	int array1[4]={p->aj_k, p->aj_d, p->cj_k, p->cj_d};
	p->vec_x_begin_gloIdx=min_vec(array1, 4);
	p->vec_x_end_gloIdx=max_vec(array1,4);
	int len2=/*p->block_k->numCol;*/p->vec_x_end_gloIdx - p->vec_x_begin_gloIdx + 1;
	p->vec_x=(MY_FLOAT_TYPE *)calloc(len2, sizeof(MY_FLOAT_TYPE));
	if(p->vec_x==NULL) {fprintf(stderr, "allocating p->vec_x fails\n");}
	for(i=0; i<len2; i++){
		p->vec_x[i]=0.0;
	}
}

void finalizePerProcDataV3(PerProcDataV3 * p)
{
	free(p->vec_y_k);
	free(p->vec_y_d);
	free(p->vec_x);
}

void printPerProcDataV3ToFile(PerProcDataV3 * p, int rank)
{
	char  filename[1024];
	sprintf(filename, "PerProcDataV3_rank_%d", rank);
	int colIdx, rowIdx, firstEntryInCol, lastEntryInCol, entryIdx;
	int firstEntryInRow, lastEntryInRow;
	int i, j;
	FILE * fp=fopen(filename, "w");
	if(fp==NULL){
		fprintf(stderr, "%d: cannot open %s\n", rank, filename);
	}
	fprintf(fp, "version=%d\n", p->version);
	fprintf(fp, "ai_k=%d, aj_k=%d, ci_k=%d, cj_k=%d\n", p->ai_k, p->aj_k, p->ci_k, p->cj_k);
	fprintf(fp, "ai_d=%d, aj_d=%d, ci_d=%d, cj_d=%d\n", p->ai_d, p->aj_d, p->ci_d, p->cj_d);
	fprintf(fp, "vec_y_begin_gloIdx_d=%d, vec_y_end_gloIdx_d=%d\n",
			p->vec_y_begin_gloIdx_d, p->vec_y_end_gloIdx_d);
	fprintf(fp, "vec_x_begin_gloIdx=%d, vec_x_end_gloIdx=%d\n",
			p->vec_x_begin_gloIdx, p->vec_x_end_gloIdx);

	//block_k
	fprintf(fp, "block_k, numRow=%d, numCol=%d, numNonzero=%d:\n",
			p->matCSC_k->numRow, p->matCSC_k->numCol, p->matCSC_k->numNonzero);
	for(colIdx=0; colIdx<p->matCSC_k->numCol; colIdx++){
		firstEntryInCol=p->matCSC_k->col_ptr[colIdx];
		lastEntryInCol=p->matCSC_k->col_ptr[colIdx+1];
		for(entryIdx=firstEntryInCol; entryIdx<lastEntryInCol; entryIdx++){
			rowIdx=p->matCSC_k->row_idx[entryIdx];
			fprintf(fp, "%d_[%d][%d]=%e\n", entryIdx, rowIdx, colIdx, p->matCSC_k->val[entryIdx]);
		}
	}
	//block_d
	fprintf(fp, "block_d, numRow=%d, numCol=%d, numNonzero=%d:\n",
			p->matCSR_d->numRow, p->matCSR_d->numCol, p->matCSR_d->numNonzero);
	for(rowIdx=0; rowIdx<p->matCSR_d->numRow; rowIdx++){
		firstEntryInRow=p->matCSR_d->row_ptr[rowIdx];
		lastEntryInRow=p->matCSR_d->row_ptr[rowIdx+1];
//		fprintf(fp, "firstEntryInRow=%d, lastEntryInRow=%d\n", firstEntryInRow, lastEntryInRow);
		for (entryIdx = firstEntryInRow; entryIdx < lastEntryInRow; entryIdx++){
			colIdx = p->matCSR_d->col_idx[entryIdx];
			fprintf(fp, "%d_[%d][%d]=%e\n", entryIdx, rowIdx, colIdx, p->matCSR_d->val[entryIdx]);
		}
	}
	//vector
	fprintf(fp, "vec_y_k, dimension=%d:\n", p->matCSC_k->numRow);
	for(i=0; i<p->matCSC_k->numRow; i++){
		fprintf(fp, "[%d]=%e\n", i, p->vec_y_k[i]);
	}
	fprintf(fp, "vec_y_d, dimension=%d, vec_y_begin_gloIdx_d=%d \n", p->matCSR_d->numRow, p->vec_y_begin_gloIdx_d);
	for(i=p->vec_y_begin_gloIdx_d; i<=p->vec_y_end_gloIdx_d; i++){
		fprintf(fp, "[%d]=%e\n", i, p->vec_y_d[i-p->vec_y_begin_gloIdx_d]);
	}
	fprintf(fp, "vec_x, dimension=%d, vec_x_begin_gloIdx=%d \n", p->vec_x_end_gloIdx-p->vec_x_begin_gloIdx+1, p->vec_x_begin_gloIdx);
	for(i=p->vec_x_begin_gloIdx; i<=p->vec_x_end_gloIdx; i++){
		fprintf(fp, "[%d]=%e\n", i, p->vec_x[i-p->vec_x_begin_gloIdx]);
	}

	fclose(fp);
}

#if 0

/*return the minimal element of a vector*/
int min_vec(int * array, int len)
{
	int min=array[0];
	int i;
	for(i=0; i<len; i++){
		if(array[i]<min)
			min=array[i];
	}
	return min;
}

/*return the maximal element of a vector*/
int max_vec(int * array, int len)
{
	int max=array[0];
	int i;
	for(i=0; i<len; i++){
		if(array[i]>max)
			max=array[i];
	}
	return max;
}

/*return the minimal element with its index of a vector
 * IN: const int * array, const int len
 * OUT: pointer: int * index, int * max
 * */
void min_vec1(const int * array, const int len, int * min_idx, int * min_val)
{
	*min_val=array[0];
	*min_idx=0;
	int i;
	for(i=0; i<len; i++){
		if(array[i]<*min_val){
			*min_val=array[i];
			*min_idx=i;
		}
	}
}


void max_vec1(const int * array, const int len, int * max_idx, int * max_val)
{
	*max_val=array[0];
	*max_idx=0;
	int i;
	for(i=0; i<len; i++){
		if(array[i]>*max_val){
			*max_val=array[i];
			*max_idx=i;
		}
	}
}

#endif


/*
 * initialize SparseMatrixCSR using a basic sparse matrix
 * basic sparse matrix must be ready before invoking
 * */
void init_SparseMatrixCSR_fromBasicSpMat2(SpMatCSR* matrixCSR,BasicSpMat2 * bSpMat2)
{
	matrixCSR->numRow = bSpMat2->numRow;
	matrixCSR->numCol = bSpMat2->numCol;
	matrixCSR->numNonzero = bSpMat2->numNonzero;

	int i;
	matrixCSR->row_ptr	= (int*)malloc(sizeof(int) * (matrixCSR->numRow + 1));
	if(matrixCSR->row_ptr==NULL){
		fprintf(stderr, "can not allocate matrixCSR->row_ptr\n"); exit(EXIT_FAILURE);
	}
	for(i=0; i< matrixCSR->numRow + 1; i++){
		matrixCSR->row_ptr[i]=0;
	}
	matrixCSR->col_idx  = (int*)malloc(sizeof(int) *(matrixCSR->numNonzero));
	if(matrixCSR->col_idx==NULL ){
		fprintf(stderr, "can not allocate matrixCSR->col_idx\n"); exit(EXIT_FAILURE);
	}
	for(i=0; i<matrixCSR->numNonzero; i++){
		matrixCSR->col_idx[i]=0;
	}
	matrixCSR->val = (MY_FLOAT_TYPE*)malloc(sizeof(MY_FLOAT_TYPE)*(matrixCSR->numNonzero));
	if(matrixCSR->val==NULL){
		fprintf(stderr, "can not allocate matrixCSR->val\n"); exit(EXIT_FAILURE);
	}
	for(i=0;i<matrixCSR->numNonzero; i++){
		matrixCSR->val[i]=0.0;
	}
}
/*
 * convert basic sparse matrix to CSR format, CSR row index here is converted to local index,
 * column index is global index, column index will be converted to local index in initPerProcDataV3()
 * also convert global rowIdx to local rowIdx by using temp1
 * 11/23/2010 fix a bug
 * add
  	int lastIndex;
  	int j;
	for(j=prevRow+1; j<matrixCSR->numRow; j++ ){
		matrixCSR->row_ptr[j]=lastIndex;
	}
 * */
void basicSpMat2TOSpMatCSR(SpMatCSR* matrixCSR, BasicSpMat2* bSpMat2, int rank)
{
	int currentRow = 0, prevRow = 0;
	int rowPointerIndex = 0;
	matrixCSR->row_ptr[0] = 0;

	//10/05/10 record which row the current partial matrix begins
	int temp1;
	int lastIndex;
	if(rank==0)
		temp1=0;//to prevent some matrix does not start from row 0 and column 0
	else
		temp1=bSpMat2->rowIdx[0]/*nonzeroEntry[0].rowIndex*/;
	int i;
	for (i = 0; i < bSpMat2->numNonzero; i++) {
		matrixCSR->col_idx[i] = bSpMat2->colIdx[i];
		matrixCSR->val[i] = bSpMat2->val[i];
		currentRow =bSpMat2->rowIdx[i] -temp1;

		if (currentRow > prevRow) {
			int j;
			for (j = prevRow + 1; j < currentRow + 1; j++) {
				matrixCSR->row_ptr[j] = lastIndex = i;
			}
			prevRow = currentRow;
		}
	}
	//11/23/2010 fill the last several rows of row_ptr, without these lines, there will be serious problem!!
	int j;
	for(j=prevRow+1; j<matrixCSR->numRow; j++ ){
		matrixCSR->row_ptr[j]=lastIndex;
	}
	matrixCSR->row_ptr[matrixCSR->numRow] = matrixCSR->numNonzero;
}

void finalizeSpMatCSR(SpMatCSR* matrixCSR) {
	free(matrixCSR->row_ptr);
	free(matrixCSR->col_idx);
	free(matrixCSR->val);
}

void printSpMatCSR(int rank, SpMatCSR* matrixCSR)
{
	int i, j;
	for(i=0; i<matrixCSR->numRow; i++){
		int start=matrixCSR->row_ptr[i], end=matrixCSR->row_ptr[i+1];
		for(j=start; j<end; j++)
		{
			printf("%d: [%d][%d]=%e \n", rank, i, matrixCSR->col_idx[j], matrixCSR->val[j]);
		}

	}

}


/* 10/3/2010
 * load damping matrix into basic sparse matrix format
 * */
void loadDampingData_ascii(const char dampingFileName[],
		BasicSpMat2 * basicSpMat_d)
{
	FILE* fp1=fopen(dampingFileName, "r");
	if (fp1==NULL) {
		fprintf(stderr, "Can not open the file: %s \n", dampingFileName);
		exit(EXIT_FAILURE);
	}

	int * counter=&(basicSpMat_d->count);
	int no_nz,//number of nonzeros
			rowIdx,//row index
			colIdx;
	double val;

	while (feof(fp1)==0) {
		int c1= fscanf(fp1, "%d %d", &no_nz, &rowIdx);
		if (c1!=2) {
			printf("c1!=2\t %d\n", rowIdx);
			break;
		}
		rowIdx--;
		int ii;
		for (ii=0; ii<no_nz; ii++) {
			int c2= fscanf(fp1, "%d %le ", &colIdx, &val);
			if(c2!=2)	 printf("c2!=2\t %d\n", counter);
			--colIdx;

			if(*counter<basicSpMat_d->numNonzero){
				basicSpMat_d->rowIdx[*counter]=rowIdx;//adjust 0-based
				basicSpMat_d->colIdx[*counter]=colIdx;
				basicSpMat_d->val[*counter]=val;
#ifdef DEBUG
				printf("rowIdx=%d, colIdx=%d, val=%e\n", rowIdx, colIdx, val);
#endif
			}
			else printf("loadDampingData_ascii: counter is greater than number of nonzero\n");
			(*counter)++;
		}
	}
	fclose(fp1);
}

void mpiio_load_damp_info(int procRank, int numProc, char damp_info_filename[], const int startRowIdx_perProc_d,
		const int numRow_perProc_d, int  nnzPerRow_d_array[])
{
	MPI_Status status;
	MPI_Offset my_current_offset;/*, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double)*/;
	MPI_File  fp;
	int err = MPI_File_open(MPI_COMM_WORLD, damp_info_filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fp);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
/*	if (file_open_error != MPI_SUCCESS) {
		char error_string[BUFSIZ];		int length_of_error_string, error_class;		MPI_Error_class(file_open_error, &error_class);		MPI_Error_string(error_class, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Error_string(file_open_error, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Abort(MPI_COMM_WORLD, file_open_error);
	}*/
	my_current_offset=sizeof(int)*startRowIdx_perProc_d;

	err = MPI_File_read_at(fp, my_current_offset, nnzPerRow_d_array, numRow_perProc_d, MPI_INT, &status);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	err = MPI_File_close(&fp);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
}

void mpiio_load_damp_once(int procRank, int numProc, char damp_binary_filename[], long long nnzPerProc_d,
	void * spMat, MATRIXTYPE info)
{
	long long i;
	int file_open_error, err=0;
/*MPI datatype*/
	MPI_Datatype MatNonzeroEntryType, oldtypes[2];
	int blockcounts[2];
	MPI_Aint offsets[2], extent, dataUnitLenByByte;
	offsets[0] = 0;
	oldtypes[0] = MPI_INT;
	blockcounts[0] = 2;
	err = MPI_Type_extent(MPI_INT, &extent);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	offsets[1] = 2 * extent;
	oldtypes[1] = MPI_DOUBLE;
	blockcounts[1] = 1;
	err = MPI_Type_struct(2, blockcounts, offsets, oldtypes, &MatNonzeroEntryType);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	err = MPI_Type_commit(&MatNonzeroEntryType);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	err = MPI_Type_extent(MatNonzeroEntryType, &dataUnitLenByByte);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	MPI_Status status;
	/*MPI IO operation*/
	MPI_Offset my_rank_offset_infile/*, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double)*/;
	MPI_File  fh_RD_kernel;
	//   MPI_Status status;


	int rowIdx, colIdx;
	err = MPI_File_open(MPI_COMM_WORLD, damp_binary_filename,
			MPI_MODE_RDONLY, MPI_INFO_NULL, &fh_RD_kernel);
//	if (file_open_error != MPI_SUCCESS) {
//		char error_string[BUFSIZ];		int length_of_error_string, error_class;		MPI_Error_class(file_open_error, &error_class);		MPI_Error_string(error_class, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Error_string(file_open_error, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Abort(MPI_COMM_WORLD, file_open_error);
//	}
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	long long global_offset=0;
	int sendBuf=0;
	int recvBuf=0;
	/* Compute offset for each MPI task
	 * rank i, first compute its offset, and send added with its data length,
	 * and then send it to rank i+1, rank i+1 get its offset, and added with its data length,
	 * and send to rank i+2
	 * ... continue until the last rank */
	//<<
	if(numProc>1){
		if(procRank==0){
			sendBuf=global_offset+nnzPerProc_d;
			MPI_Send(&sendBuf, 1, MPI_INT, procRank+1, 0, MPI_COMM_WORLD);
		}
		else {
			MPI_Recv(&recvBuf, 1, MPI_INT, procRank-1, 0, MPI_COMM_WORLD, &status);
			global_offset=recvBuf;
			if(procRank<numProc-1){
				sendBuf=global_offset+nnzPerProc_d;
				MPI_Send(&sendBuf, 1, MPI_INT, procRank+1, 0, MPI_COMM_WORLD);
			}
		}
	}
	//>>
	MPI_Barrier(MPI_COMM_WORLD);

	const int MPIIO_CHUNK_SIZE=10000000;
//	printf("%d: mpiio_load_damp: global_offset=%d \n", procRank, global_offset);

/*	if(nnzPerProc_d>MPIIO_CHUNK_SIZE){
		fprintf(stderr, "%d: WARNING(%s(%d)-%s): nnzPerProc=%d is too large. It can potentially break MPI_File_read_at. Advice: use more MPI tasks to reduce it\n",
				procRank, __FILE__,__LINE__,__FUNCTION__, nnzPerProc_d);
	}*/
	my_rank_offset_infile = global_offset * (dataUnitLenByByte);
	NONZERO * nonzeroSet = (NONZERO *)valloc(sizeof(NONZERO)*nnzPerProc_d);
	memset(nonzeroSet, 0, sizeof(NONZERO) * nnzPerProc_d);



	long long local_offset=0;
	for(local_offset=0; local_offset<nnzPerProc_d; local_offset+=MPIIO_CHUNK_SIZE){
		MPI_Offset my_local_offset_infile = my_rank_offset_infile + local_offset * dataUnitLenByByte;
		if(local_offset + MPIIO_CHUNK_SIZE > nnzPerProc_d){//last iteration, less than MPIIO_CHUNK_SIZE
			err=MPI_File_read_at(fh_RD_kernel, my_local_offset_infile, nonzeroSet+local_offset,
					nnzPerProc_d-local_offset, MatNonzeroEntryType, &status);
		}else{
			err=MPI_File_read_at(fh_RD_kernel, my_local_offset_infile, nonzeroSet+local_offset,
					MPIIO_CHUNK_SIZE, MatNonzeroEntryType, &status);
		}
		if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
#ifdef DEBUG
		printf("%s(%d)-%s: \t rank=%d: MPI_File_read_at %dst done. global_offset=%lld, my_local_offset_infile=%lld, MPIIO_CHUNK_SIZE=%d, nnzPerProc_d=%lld, nnzeroSet[local_offset=%d]:[%d,%d]=%le\n",
				__FILE__,__LINE__,__FUNCTION__, procRank, local_offset/MPIIO_CHUNK_SIZE, global_offset, my_local_offset_infile, MPIIO_CHUNK_SIZE, nnzPerProc_d, local_offset, nonzeroSet[local_offset].r, nonzeroSet[local_offset].c, nonzeroSet[local_offset].val);
#endif
	}


//	err = MPI_File_read_at(fh_RD_kernel, my_rank_offset_infile, nonzeroSet, nnzPerProc_d, MatNonzeroEntryType, &status);
//	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);


	err = MPI_Type_free(&MatNonzeroEntryType);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	err = MPI_File_close(&fh_RD_kernel);
	if(err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
//----------------------------------------------------------------------------------------------


	//	store into CSR format
	if(info==CSR){
		SpMatCSR * spMatCSR = (SpMatCSR *)spMat;
		int preRow=0;
		int temp1;
		if(procRank==0)temp1=0;//to prevent some matrix does not start from row 0 and column 0
		else temp1=nonzeroSet[0].r-1;
		int j=0;
		int * nnzPerRow= (int * )malloc(sizeof(int)*spMatCSR->numRow);
		memset(nnzPerRow, 0, sizeof(int)*spMatCSR->numRow);
		for(i=0; i<nnzPerProc_d; i++){
			nonzeroSet[i].r--; nonzeroSet[i].c--;//adjust to 0-based indexing
			int nowRow=nonzeroSet[i].r - temp1;
			nnzPerRow[nowRow]++;
			spMatCSR->col_idx[i]=nonzeroSet[i].c;
			spMatCSR->val[i]=nonzeroSet[i].val;
			//err check
			if(0 <= nowRow && nowRow < spMatCSR->numRow && 0 <= nonzeroSet[i].c && nonzeroSet[i].c< spMatCSR->numCol){}
			else{
				fprintf(stderr, "%s(%d)-%s: \t rank=%d: nowRow=%d matCSR_d->numRow=%d matCSR_d->numCol=%d [%d %d]=%e\n",
						__FILE__,__LINE__,__FUNCTION__, procRank, nowRow, spMatCSR->numRow, spMatCSR->numCol, nonzeroSet[i].r, nonzeroSet[i].c, nonzeroSet[i].val);				exit(-1);
			}
#ifdef DEBUG
	//		printf("%d: nonzero[%d]= %d %d %e\n", procRank, i, nonzeroSet[i].r, nonzeroSet[i].c, nonzeroSet[i].val);
#endif
		}
		spMatCSR->row_ptr[0]=0;
		for(i=0; i<spMatCSR->numRow; i++){
			spMatCSR->row_ptr[i+1]=spMatCSR->row_ptr[i]+nnzPerRow[i];
		}
		free(nnzPerRow);
	}else if (info==CSC){
		SpMatCSC * spMatCSC = (SpMatCSC *)spMat;
		int preCol=0;
		int temp1;
		if(procRank==0)temp1=0;//to prevent some matrix does not start from row 0 and column 0
		else temp1=nonzeroSet[0].c-1;
		int j=0;
		int * nnzPerCol=(int *)malloc(sizeof(int)*spMatCSC->numCol);
		memset(nnzPerCol, 0, sizeof(int)*spMatCSC->numCol);
		for(i=0; i<nnzPerProc_d; i++){
			nonzeroSet[i].r--; nonzeroSet[i].c--;//adjust to 0-based indexing
			int nowCol=nonzeroSet[i].c - temp1;
			nnzPerCol[nowCol]++;
			spMatCSC->row_idx[i]=nonzeroSet[i].r;
			spMatCSC->val[i]=nonzeroSet[i].val;
			if(0<=nowCol && nowCol<spMatCSC->numCol && 0<= nonzeroSet[i].r && nonzeroSet[i].r<spMatCSC->numRow);
			else{
				fprintf(stderr, "%s(%d)-%s: \t rank=%d: nowCol=%d matCSC_d->numCol=%d matCSC_d->numRow=%d [%d %d]=%e\n",
					__FILE__,__LINE__,__FUNCTION__, procRank, nowCol, spMatCSC->numCol, spMatCSC->numRow, nonzeroSet[i].r, nonzeroSet[i].c, nonzeroSet[i].val);				exit(-1);
			}
		}
		spMatCSC->col_ptr[0]=0;
		for(i=0; i<spMatCSC->numCol; i++){
			spMatCSC->col_ptr[i+1]=spMatCSC->col_ptr[i]+nnzPerCol[i];
		}
		free(nnzPerCol);
	}

	free(nonzeroSet);

	return ;
}


//Not finished yet
//void mpiio_load_damp_multi(int procRank, int numProc, char damp_binary_filename[], int nnzPerProc_d,
//	SpMatCSR * matCSR_d	)
//{
//	int i;
///*	typedef struct{
//		int r, c;
//		double val;
//	}NONZERO;*/
//
///*MPI datatype*/
//	MPI_Datatype MatNonzeroEntryType, oldtypes[2];
//	int blockcounts[2];
//	MPI_Aint offsets[2], extent, dataUnitLenByByte;
//	offsets[0] = 0;
//	oldtypes[0] = MPI_INT;
//	blockcounts[0] = 2;
//	MPI_Type_extent(MPI_INT, &extent);
//	offsets[1] = 2 * extent;
//	oldtypes[1] = MPI_DOUBLE;
//	blockcounts[1] = 1;
//	MPI_Type_struct(2, blockcounts, offsets, oldtypes, &MatNonzeroEntryType);
//	MPI_Type_commit(&MatNonzeroEntryType);
//	MPI_Type_extent(MatNonzeroEntryType, &dataUnitLenByByte);
//	if(procRank==0)
//		printf("%d: dataUnitLenByByte(MatNonzeroEntryType: int + int + double)=%d bytes\n", procRank, dataUnitLenByByte); //bytes
//
//	MPI_Status status;
//	/*MPI IO operation*/
//	MPI_Offset my_current_offset/*, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double)*/;
//	MPI_File  fh_RD_kernel;
//	//   MPI_Status status;
//	int file_open_error;
//	int rowIdx, colIdx;
//	file_open_error = MPI_File_open(MPI_COMM_WORLD, damp_binary_filename,
//			MPI_MODE_RDONLY, MPI_INFO_NULL, &fh_RD_kernel);
//	if (file_open_error != MPI_SUCCESS) {
//		char error_string[BUFSIZ];		int length_of_error_string, error_class;		MPI_Error_class(file_open_error, &error_class);		MPI_Error_string(error_class, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Error_string(file_open_error, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Abort(MPI_COMM_WORLD, file_open_error);
//	}
//
//	int offset1=0;
//	int sendBuf=0;
//	int recvBuf=0;
//	if(numProc>1){
//		if(procRank==0){
//			sendBuf=offset1+nnzPerProc_d;
//			MPI_Send(&sendBuf, 1, MPI_INT, procRank+1, 0, MPI_COMM_WORLD);
//		}
//		else {
//			MPI_Recv(&recvBuf, 1, MPI_INT, procRank-1, 0, MPI_COMM_WORLD, &status);
//			offset1=recvBuf;
//			if(procRank<numProc-1){
//				sendBuf=offset1+nnzPerProc_d;
//				MPI_Send(&sendBuf, 1, MPI_INT, procRank+1, 0, MPI_COMM_WORLD);
//			}
//		}
//	}

//	printf("%d: mpiio_load_damp: offset1=%d \n", procRank, offset1);
//	MPI_Barrier(MPI_COMM_WORLD);
//
//	my_current_offset = offset1 * (dataUnitLenByByte);
//	NONZERO * nonzeroSet = (NONZERO *)malloc(sizeof(NONZERO)*nnzPerProc_d);
//
//	printf("%d: %s(%d)-%s: \t damp MPI_File_read_at begin. offset1=%lld, my_current_offset=%lld, nnzPerProc_d=%lld\n", procRank, __FILE__,__LINE__,__FUNCTION__, offset1, my_current_offset, nnzPerProc_d);
//
//
//	MPI_File_read_at(fh_RD_kernel, my_current_offset, nonzeroSet, nnzPerProc_d, MatNonzeroEntryType, &status);
////	store into CSR format
//	int preRow=0;
//	int temp1;
//	if(procRank==0)temp1=0;//to prevent some matrix does not start from row 0 and column 0
//	else temp1=nonzeroSet[0].r-1;
//	int j=0;
//	int * nnzPerRow= (int * )valloc(sizeof(int)*matCSR_d->numRow);
//	memset(nnzPerRow, 0, sizeof(int)*matCSR_d->numRow);
//	for(i=0; i<nnzPerProc_d; i++){
//		nonzeroSet[i].r--; nonzeroSet[i].c--;//adjust to 0-based indexing
//		int nowRow=nonzeroSet[i].r - temp1;
//		nnzPerRow[nowRow]++;
//		matCSR_d->col_idx[i]=nonzeroSet[i].c;
//		matCSR_d->val[i]=nonzeroSet[i].val;
//		//err check
//		if(0 <= nowRow < matCSR_d->numRow && 0 <= nonzeroSet[i].c < matCSR_d->numCol){}
//		else{
//			fprintf(stderr, "%s(%d)-%s: \t %d: nowRow=%d matCSR_d->numRow=%d matCSR_d->numCol=%d [%d %d]=%e\n",
//					__FILE__,__LINE__,__FUNCTION__, procRank, nowRow, matCSR_d->numRow, matCSR_d->numCol, nonzeroSet[i].r, nonzeroSet[i].c, nonzeroSet[i].val);
//			exit(-1);
//		}
//#ifdef DEBUG
////		printf("%d: nonzero[%d]= %d %d %e\n", procRank, i, nonzeroSet[i].r, nonzeroSet[i].c, nonzeroSet[i].val);
//#endif
//	}
//	matCSR_d->row_ptr[0]=0;
//	for(i=0; i<matCSR_d->numRow; i++){
//		matCSR_d->row_ptr[i+1]=matCSR_d->row_ptr[i]+nnzPerRow[i];
//	}
//	free(nnzPerRow);
//	free(nonzeroSet);
//	MPI_Type_free(&MatNonzeroEntryType);
//	MPI_File_close(&fh_RD_kernel);
//	return ;
//}

/*count the number of rows and total number of nonzeros of the damping data
 * input: const char dampingFileName[], const int numRow
 * output:  int  numNonzeroPerRow_d[], int * totalNumNonzero*/
void countDampingData_ascii(const char dampingFileName[], const int numRow,
		int  numNonzeroPerRow_d[], int * totalNumNonzero)
{
/*	*numRow=0;*/
	*totalNumNonzero=0;
	FILE* fp1=fopen(dampingFileName, "r");
	if (fp1==NULL) {
		fprintf(stderr, "Can not open the file: %s \n", dampingFileName);
		exit(EXIT_FAILURE);
	}
	int no_nz,//number of nonzeros
			rowIdx,//row index
			colIdx;
	MY_FLOAT_TYPE val;
	while (feof(fp1)==0) {
		int c1= fscanf(fp1, "%d %d", &no_nz, &rowIdx);
		if (c1!=2) {
			printf("countDampingData_ascii c1!=2\t rowIdx=%d\n", rowIdx);
			break;
		}
		numNonzeroPerRow_d[--rowIdx]=no_nz;//adjust to 0-based index
		int ii;
		for (ii=0; ii<no_nz; ii++) {
#if SDVERSION == 1
			int c2= fscanf(fp1, "%d %e", &colIdx, &val);
#elif SDVERSION == 2
			int c2= fscanf(fp1, "%d %le", &colIdx, &val);
#endif
			if(c2!=2)
			 fprintf(stderr, "countDampingData_ascii c2!=2\t \n");
		}
		(*totalNumNonzero)+=no_nz;
	}
/*	*numRow=rowIdx;*/
	fclose(fp1);
}

/*
 * read column permutation generated by damping RCM reordering
 * file format
 * 2 current column 0 is 2 in the original column
 * 5
 * ...
 * */
void read_col_perm(char  filename [], int colPerm[], int numCol)
{
	FILE * fp =fopen(filename, "r");
	if(fp==NULL)
		fprintf(stderr, "%s(%d)-%s: cannot open %s\n", __FILE__,__LINE__,__FUNCTION__, filename);
	int i;
	for(i=0; i<numCol; i++){
		if(feof(fp)==0){
			fscanf(fp, "%d", &colPerm[i]);
			colPerm[i]--;//adjust to 0-based matrix index
		}
	}
	fclose(fp);

/*
#ifdef DEBUG
	printf("Column Permutation: \n");
	for(i=0; i<numCol; i++){
		printf("%d\t",colPerm[i]);
	}
	printf("end of Column Permutation \n");
#endif
*/
}


/*
 * IN: char * kernel_col_info_filename, int numCol,
 * OUT:	KernelInfo * kernelColInfo
 * adjust to 0-based matrix index
 * */
void read_kernel_col_info(int procRank, char * kernel_col_info_filename,
		int numCol, KernelColInfoPerCol * kernelColInfo) {
	int i;
	//read kernel info file,,,,,,,,,,,,,,,,(((((((((((((((((((((
	FILE * fp_info = fopen(kernel_col_info_filename, "r");
	if (fp_info == NULL) {
		fprintf(stderr, "Cannot open the file: %s  \n",
				kernel_col_info_filename);
		exit(-1);
	}

	for (i = 0; i < numCol; i++) {
		fscanf(fp_info, "%d %d %lld\n", &kernelColInfo[i].colIdx,
				&kernelColInfo[i].nnz,
				&kernelColInfo[i].globalBeginIdx);
		kernelColInfo[i].colIdx--;// adjust to 0-based from 1-based matrix
		kernelColInfo[i].globalBeginIdx--;// adjust to 0-based from 1-based matrix
	}

/*
#ifdef DEBUG
	if (procRank == 0)
		printf("%d %d %d\n", kernelColInfo[i].colIdx,
				kernelColInfo[i].nnz,
				kernelColInfo[i].globalBeginIdx);
#endif
*/

	fclose(fp_info);
	/////////)))))))))))))))))))))
}

/* 9/19 result is all zero when using Double; no problem for float
 * 8/3/2011
 * Load kernel (column major) to CSC format
 *
 *
 * This version doesn't have column permutation
 * One MPI_file_read_at read a column
 * */
void mpiio_load_kernelColBin2CSC_multi(int procRank, int numProc,	char * kernel_filename,
		long long nnzPerProc_k, KernelColInfoPerCol kernelColInfo_perProc[], SpMatCSC * spMatCSC)
{
	MPI_Status status;
	/*MPI IO operation*/
	MPI_Offset my_current_offset/*, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double)*/;
	MPI_File  fh_RD_kernel;

	int file_open_error;
//	int rowIdx, colIdx;
	int i, j;
/*	typedef struct{
		int r, c;
		double val;
	}NONZERO;*/
/*MPI datatype*/
	MPI_Datatype MatNonzeroEntryType, oldtypes[2];
	int blockcounts[2];
	MPI_Aint offsets[2], extent, dataUnitLenByByte;
	offsets[0] = 0;
	oldtypes[0] = MPI_INT;
	blockcounts[0] = 2;
	MPI_Type_extent(MPI_INT, &extent);
	offsets[1] = 2 * extent;
	oldtypes[1] = MPI_DOUBLE;
	blockcounts[1] = 1;
	MPI_Type_struct(2, blockcounts, offsets, oldtypes, &MatNonzeroEntryType);
	MPI_Type_commit(&MatNonzeroEntryType);

	MPI_Type_extent(MatNonzeroEntryType, &dataUnitLenByByte);

	if(procRank==0)
		printf("%d: %s(%d)-%s: dataUnitLenByByte(MatNonzeroEntryType: int + int + double)=%d bytes\n", procRank, __FILE__,__LINE__,__FUNCTION__, dataUnitLenByByte); //bytes

	//open kernel file
	file_open_error = MPI_File_open(MPI_COMM_WORLD, kernel_filename,
			MPI_MODE_RDONLY, MPI_INFO_NULL, &fh_RD_kernel);
	if (file_open_error != MPI_SUCCESS) {
		char error_string[BUFSIZ];		int length_of_error_string, error_class;		MPI_Error_class(file_open_error, &error_class);		MPI_Error_string(error_class, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Error_string(file_open_error, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Abort(MPI_COMM_WORLD, file_open_error);
	}

	long long offset1=0;//jump to the first nonzero column
	for(i=0; i< spMatCSC->numCol; i++){
		if(kernelColInfo_perProc[i].nnz>0){
			offset1= kernelColInfo_perProc[i].globalBeginIdx;
			break;
		}
	}
	my_current_offset = offset1 * (dataUnitLenByByte);
	NONZERO * nonzeroSet/* = (NONZERO *)malloc(sizeof(NONZERO) * nnzPerProc_k)*/;
/*	MPI_File_read_at(fh_RD_kernel, my_current_offset, nonzeroSet, nnzPerProc_k, MatNonzeroEntryType, &status);*/
	printf("%d: %s(%d)-%s: \t  kernel: MPI_File_read_at begin. offset1=%lld, my_current_offset=%lld, nnzPerProc_k=%lld\n", procRank, __FILE__,__LINE__,__FUNCTION__, offset1, my_current_offset, nnzPerProc_k);


	int totalIdxOfCSC=0;
	spMatCSC->col_ptr[0]=0;
	for(i=0; i<spMatCSC->numCol; i++){
		int nnz = kernelColInfo_perProc[i].nnz;//nnz in current column
		spMatCSC->col_ptr[i+1]=spMatCSC->col_ptr[i] + nnz;
		if(nnz>0){
			nonzeroSet=(NONZERO *)malloc(sizeof(NONZERO) * nnz);
			//read a column
			MPI_File_read_at(fh_RD_kernel, my_current_offset, nonzeroSet, nnz, MatNonzeroEntryType, &status);
			my_current_offset += (nnz * dataUnitLenByByte);
			for(j=0; j<nnz; j++){
				if(totalIdxOfCSC > spMatCSC->numNonzero){//error check
					fprintf(stderr, "%s(%d)-%s: \t %d: totalIdxOfCSC=%d >= numNonzeroPerProc=%d\n", __FILE__,__LINE__,__FUNCTION__, procRank, totalIdxOfCSC, spMatCSC->numNonzero);
					exit(-1);
				}
				else if ( nonzeroSet[j].r> spMatCSC->numRow){//error check
					fprintf(stderr, "%s(%d)-%s: \t %d: [nonzeroSet[%d].r=%d > spMatCSC->numRow=%d], colIdx=%d, val=%le, nnzPerProc_k=%lld \n",
							__FILE__,__LINE__,__FUNCTION__, procRank, j, nonzeroSet[j].r, spMatCSC->numRow,
							nonzeroSet[j].c, nonzeroSet[j].val,   nnzPerProc_k	);
					exit(-1);
				}
				nonzeroSet[j].r--;
				nonzeroSet[j].c--;
				spMatCSC->row_idx[totalIdxOfCSC]=nonzeroSet[j].r;
				spMatCSC->val[totalIdxOfCSC]=(MY_FLOAT_TYPE)nonzeroSet[j].val;
				printf("%d: %s(%d)-%s: spMatCSC %le\n", procRank, __FILE__,__LINE__,__FUNCTION__, spMatCSC->val[totalIdxOfCSC]);
				totalIdxOfCSC++;

			}
			free(nonzeroSet);
		}
	}

/*	free(nonzeroSet);*/
	MPI_Type_free(&MatNonzeroEntryType);
	MPI_File_close(&fh_RD_kernel);

}

/* Only one MPI_File_read_at, read a bunch of column once
 *
 * */
void mpiio_load_kernelColBin2CSC_once(int procRank, int numProc,	char * kernel_filename,
		long long nnzPerProc_k, KernelColInfoPerCol kernelColInfo_perProc[], SpMatCSC * spMatCSC)
{
	MPI_Status status;
	/*MPI IO operation*/
	MPI_Offset my_rank_offset_infile/*, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double)*/;
	MPI_File  fh_RD_kernel;


//	int rowIdx, colIdx;
	long long i, j;
	int err=0;
/*MPI datatype*/
	MPI_Datatype MatNonzeroEntryType, oldtypes[2];
	int blockcounts[2];
	MPI_Aint offsets[2], extent, dataUnitLenByByte;
	offsets[0] = 0;
	oldtypes[0] = MPI_INT;
	blockcounts[0] = 2;
	err = MPI_Type_extent(MPI_INT, &extent);
	if (err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	offsets[1] = 2 * extent;
	oldtypes[1] = MPI_DOUBLE;
	blockcounts[1] = 1;
	err = MPI_Type_struct(2, blockcounts, offsets, oldtypes, &MatNonzeroEntryType);
	if (err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	err = MPI_Type_commit(&MatNonzeroEntryType);
	if (err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	err = MPI_Type_extent(MatNonzeroEntryType, &dataUnitLenByByte);
	if (err != MPI_SUCCESS) PARSE_MPI_ERROR(err);

	if(procRank==0)
		printf("%d: %s(%d)-%s: dataUnitLenByByte(MatNonzeroEntryType: int + int + double)=%d bytes\n", procRank, __FILE__,__LINE__,__FUNCTION__, dataUnitLenByByte); //bytes


	err = MPI_File_open(MPI_COMM_WORLD, kernel_filename,
			MPI_MODE_RDONLY, MPI_INFO_NULL, &fh_RD_kernel);
	if (err != MPI_SUCCESS) PARSE_MPI_ERROR(err);

	//printf("%s(%d)-%s: \t rank=%d: after MPI file open\n", __FILE__,__LINE__,__FUNCTION__, procRank);
	//read a bunch of column data<<
	long long global_offset=0;//
	for(i=0; i < spMatCSC->numCol; i++){
		if(kernelColInfo_perProc[i].nnz>0){
			global_offset= kernelColInfo_perProc[i].globalBeginIdx;
			break;
		}
	}
	my_rank_offset_infile = global_offset * (dataUnitLenByByte);

	const int MPIIO_CHUNK_SIZE=10000000;
/*	if(nnzPerProc_k>MPIIO_CHUNK_SIZE){
		fprintf(stderr, "%d: WARNING(%s(%d)-%s): nnzPerProc=%d is too large. It can potentially break MPI_File_read_at. Advice: use more MPI tasks to reduce it\n",
				procRank, __FILE__,__LINE__,__FUNCTION__, nnzPerProc_k);
	}*/
	NONZERO * nonzeroSet = (NONZERO *)malloc(sizeof(NONZERO) * nnzPerProc_k);
	if(NULL==nonzeroSet){
		printf("%s(%d)-%s: \t rank=%d: \n", __FILE__,__LINE__,__FUNCTION__, procRank);exit(-1);
	}
	memset(nonzeroSet, 0, sizeof(NONZERO) * nnzPerProc_k);


	long long local_offset=0;
	for(local_offset=0; local_offset<nnzPerProc_k; local_offset+=MPIIO_CHUNK_SIZE){
		MPI_Offset my_local_offset_infile = my_rank_offset_infile + local_offset * dataUnitLenByByte;
		if(local_offset + MPIIO_CHUNK_SIZE > nnzPerProc_k){//last iteration, less than MPIIO_CHUNK_SIZE
			err=MPI_File_read_at(fh_RD_kernel, my_local_offset_infile, nonzeroSet+local_offset,
					nnzPerProc_k-local_offset, MatNonzeroEntryType, &status);
		}else{
			err=MPI_File_read_at(fh_RD_kernel, my_local_offset_infile, nonzeroSet+local_offset,
					MPIIO_CHUNK_SIZE, MatNonzeroEntryType, &status);
		}
		if(err!=MPI_SUCCESS) PARSE_MPI_ERROR(err);
#ifdef DEBUG 
		printf("%s(%d)-%s: \t rank=%d: MPI_File_read_at %dst done. global_offset=%lld, my_local_offset_infile=%lld, MPIIO_CHUNK_SIZE=%d, nnzPerProc_k=%lld, nnzeroSet[local_offset=%d]:[%d,%d]=%le\n",
				__FILE__,__LINE__,__FUNCTION__, procRank, local_offset/MPIIO_CHUNK_SIZE, global_offset, my_local_offset_infile, MPIIO_CHUNK_SIZE, nnzPerProc_k, local_offset, nonzeroSet[local_offset].r, nonzeroSet[local_offset].c, nonzeroSet[local_offset].val);
#endif
	}

	//read a bunch of column data>>

	int totalIdxOfCSC=0;
	spMatCSC->col_ptr[0]=0;
	for(i=0; i<spMatCSC->numCol; i++){
		spMatCSC->col_ptr[i+1]=spMatCSC->col_ptr[i] + kernelColInfo_perProc[i].nnz;

		for(j=0; j<kernelColInfo_perProc[i].nnz; j++){
			if(totalIdxOfCSC > spMatCSC->numNonzero){//error check
				fprintf(stderr, "totalIdxOfCSC=%d >= numNonzeroPerProc=%d\n", totalIdxOfCSC, spMatCSC->numNonzero);
				exit(-1);
			}
			nonzeroSet[totalIdxOfCSC].r--;//adjust to 0-based indexing
			nonzeroSet[totalIdxOfCSC].c--;
			int r0;
			r0=spMatCSC->row_idx[totalIdxOfCSC]=nonzeroSet[totalIdxOfCSC].r;
			if (r0 >= spMatCSC->numRow){//error check
				fprintf(stderr, "%s(%d)-%s: \t %d: [nonzeroSet[%d].r=%d > spMatCSC->numRow=%d], colIdx=%d, val=%le, nnzPerProc_k=%lld \n",
						__FILE__,__LINE__,__FUNCTION__, procRank, totalIdxOfCSC, r0, spMatCSC->numRow,
						nonzeroSet[totalIdxOfCSC].c, nonzeroSet[totalIdxOfCSC].val,   nnzPerProc_k	);
				exit(-1);
			}
			spMatCSC->val[totalIdxOfCSC]=nonzeroSet[totalIdxOfCSC].val;
			totalIdxOfCSC++;
		}
	}

	free(nonzeroSet);
	err = MPI_Type_free(&MatNonzeroEntryType);
	if (err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
	err = MPI_File_close(&fh_RD_kernel);
	if (err != MPI_SUCCESS) PARSE_MPI_ERROR(err);
}

/* Updated 9/16/2011
 *	load binary column based file into reordered Compressed Sparse Column format
 *
 *	input file kernel_filename:
 (int, row index) + (int, column index)+ (double, double value)
 4 8 7.708820e-01
 5 8 9.082630e-01
 3 10 2.271540e-01
 7 25 6.604270e-01
 1 26 6.365470e-01
 *
 * notice: these below part is not efficient, solution: wrap the data into a new MPI_Datatype,
 * and read a bunch of every time
 MPI_File_read_at_all(fh_RD, my_begin_offset, &rowIdx, 1, MPI_INT, &status);
 my_begin_offset+=sizeof(int);
 MPI_File_read_at_all(fh_RD, my_begin_offset, &colIdx, 1, MPI_INT, &status);
 my_begin_offset+=sizeof(int);
 MPI_File_read_at_all(fh_RD, my_begin_offset, &val, 1, MPI_DOUBLE, &status);
 my_begin_offset+=sizeof(double);

 INPUT: 1-based index

 Use column permutation
 * */
void mpiio_load_kernelColBin2ReorderedCSC(int procRank, int numProc,	char * kernel_filename, int numCol,
		KernelColInfoPerCol  kernelColInfo[], int  colPerm_perProc[], SpMatCSC * spMatCSC)
{
	MPI_Status status;
	/*MPI IO operation*/
	MPI_Offset my_current_offset/*, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double)*/;
	MPI_File  fh_RD_kernel;
	//   MPI_Status status;
	int file_open_error;
	int rowIdx, colIdx;
	double val;
	int i, j;

/*	typedef struct{
		int r, c;
		double val;
	}NONZERO;*/

/*MPI datatype*/
	MPI_Datatype MatNonzeroEntryType, oldtypes[2];
	int blockcounts[2];
	MPI_Aint offsets[2], extent, dataUnitLenByByte;
	offsets[0] = 0;
	oldtypes[0] = MPI_INT;
	blockcounts[0] = 2;
	MPI_Type_extent(MPI_INT, &extent);
	offsets[1] = 2 * extent;
	oldtypes[1] = MPI_DOUBLE;
	blockcounts[1] = 1;
	MPI_Type_struct(2, blockcounts, offsets, oldtypes, &MatNonzeroEntryType);
	MPI_Type_commit(&MatNonzeroEntryType);

	MPI_Type_extent(MatNonzeroEntryType, &dataUnitLenByByte);

	if(procRank==0)
		printf("%d: %s(%d)-%s: dataUnitLenByByte=%d\n", procRank, __FILE__,__LINE__,__FUNCTION__, dataUnitLenByByte);

#ifdef DEBUG
	char filename2[FILENAMELEN];
	sprintf(filename2, "rank_%d.mpiioRead", procRank);
	FILE* fp_debug = fopen(filename2, "w");
#endif
	int totalIdxOfCSC=0;

	spMatCSC->col_ptr[0]=0;

	//read kernel file
	file_open_error = MPI_File_open(MPI_COMM_WORLD, kernel_filename,
			MPI_MODE_RDONLY, MPI_INFO_NULL, &fh_RD_kernel);
	if (file_open_error != MPI_SUCCESS) {
		char error_string[BUFSIZ];		int length_of_error_string, error_class;		MPI_Error_class(file_open_error, &error_class);		MPI_Error_string(error_class, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Error_string(file_open_error, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Abort(MPI_COMM_WORLD, file_open_error);
	}

	//read MPIIO file and store it in CSC format
	for(i=0; i<spMatCSC->numCol; i++){
		KernelColInfoPerCol * currentCol = &(kernelColInfo[colPerm_perProc[i]]);
		//construct col_ptr
		spMatCSC->col_ptr[i+1]=spMatCSC->col_ptr[i] + currentCol->nnz;
		//read one column per read
		if(totalIdxOfCSC > spMatCSC->numNonzero){
			fprintf(stderr, "%s(%d)-%s: totalIdxOfCSC=%d >= numNonzeroPerProc=%d\n", __FILE__,__LINE__,__FUNCTION__, totalIdxOfCSC, spMatCSC->numNonzero);
			exit(-1);
		}
		if(currentCol->nnz >0){
			long long offset1 = currentCol->globalBeginIdx;
			my_current_offset = offset1 * (dataUnitLenByByte);
			NONZERO * nonzeroSet = (NONZERO *)calloc(currentCol->nnz, sizeof(NONZERO));
			//read all nonzeros in a column every time
//			printf("%d: %s(%d)-%s: \t  kernel: MPI_File_read_at begin. offset1=%lld, my_current_offset=%lld \n", procRank, __FILE__,__LINE__,__FUNCTION__, offset1, my_current_offset);
			MPI_File_read_at(fh_RD_kernel, my_current_offset, nonzeroSet, currentCol->nnz, MatNonzeroEntryType, &status);

			for(j=0; j<currentCol->nnz; j++){
				nonzeroSet[j].r--;
				nonzeroSet[j].c--;
				spMatCSC->row_idx[totalIdxOfCSC]=nonzeroSet[j].r;
				spMatCSC->val[totalIdxOfCSC]=(MY_FLOAT_TYPE)nonzeroSet[j].val;
	//#ifdef DEBUG
//				printf("spMatCSC->row_idx[%d]=%d, spMatCSC->val[%d]=%e\n", totalIdxOfCSC, spMatCSC->row_idx[totalIdxOfCSC], totalIdxOfCSC, spMatCSC->val[totalIdxOfCSC]);
	//#endif
				totalIdxOfCSC++;
			}
			free(nonzeroSet);
		}

	}

	MPI_Type_free(&MatNonzeroEntryType);

#ifdef DEBUG
	MPI_Offset total_number_of_bytes;
	MPI_File_get_size(fh_RD_kernel, &total_number_of_bytes);
	printf("%d: total_number_of_bytes=%d\n", procRank,(int) total_number_of_bytes);

#endif

/*
#ifdef DEBUG
	fprintf(fp_debug, "print CSC matrix column ptr\n");
	for (i = 0; i <= spMatCSC->numCol; i++) {
		fprintf(fp_debug, "col_ptr[%d]=%d\t",i, spMatCSC->col_ptr[i]);
	}
	fprintf(fp_debug, "\n");
	for (i = 0; i < spMatCSC->numCol; i++) {
		int j;
		int start=spMatCSC->col_ptr[i];
		int end=spMatCSC->col_ptr[i+1];
		for (j=start; j<end; j++){
			fprintf(fp_debug, "[%d][%d] = %e \n", spMatCSC->row_idx[j], i, spMatCSC->val[j]);
		}
	}
	fflush(NULL);//flush all output files
#endif
*/
#ifdef DEBUG
	fclose(fp_debug);
#endif
	MPI_File_close(&fh_RD_kernel);
	return;
}

//void mpiio_load_kernelColBin2ReorderedCSC(int procRank, int numProc,	char * kernel_filename, int numCol,
//		KernelColInfoPerCol  kernelColInfo[], int  colPerm_perProc[], SpMatCSC * spMatCSC)
//{
//	MPI_Status status;
//	/*MPI IO operation*/
//	MPI_Offset my_current_offset, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double);
//	MPI_File  fh_RD_kernel;
//	//   MPI_Status status;
//	int file_open_error;
//	int rowIdx, colIdx;
//	double val;
//	int i;
//#ifdef DEBUG
//	char filename2[FILENAMELEN];
//	sprintf(filename2, "rank_%d.mpiioRead", procRank);
//	FILE* fp_debug = fopen(filename2, "w");
//#endif
//	int totalIdxOfCSC=0;
//
//	spMatCSC->col_ptr[0]=0;
//
//	//read kernel file
//	file_open_error = MPI_File_open(MPI_COMM_WORLD, kernel_filename,
//			MPI_MODE_RDONLY, MPI_INFO_NULL, &fh_RD_kernel);
//	if (file_open_error != MPI_SUCCESS) {
//		char error_string[BUFSIZ];		int length_of_error_string, error_class;		MPI_Error_class(file_open_error, &error_class);		MPI_Error_string(error_class, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Error_string(file_open_error, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Abort(MPI_COMM_WORLD, file_open_error);
//	}
//	//read MPIIO file and store it in CSC format
//	for(i=0; i<spMatCSC->numCol; i++){
//		//construct col_ptr
//		spMatCSC->col_ptr[i+1]=spMatCSC->col_ptr[i]+kernelColInfo[colPerm_perProc[i]].nnz;
////		printf("col_ptr[%d+1]=%d\n", i, spMatCSC->col_ptr[i+1]);
//		int j;
//		for(j =0; j<kernelColInfo[colPerm_perProc[i]].nnz; j++){
//			if(totalIdxOfCSC>=spMatCSC->numNonzero){
//				fprintf(stderr, "totalIdxOfCSC=%d >= numNonzeroPerProc=%d\n", totalIdxOfCSC, spMatCSC->numNonzero);
//				exit(-1);
//			}
//			int index = kernelColInfo[colPerm_perProc[i]].globalBeginIdx + j;
//			my_current_offset = index * (dataUnitLenByByte);
//#ifdef DEBUG
//			fprintf(fp_debug, "%d: index=%d my_current_offset=%d,\n",
//									procRank, index, (int) my_current_offset);
//#endif
//			MPI_File_read_at(fh_RD_kernel, my_current_offset, &rowIdx, 1,
//					MPI_INT, &status);
//			rowIdx--;//0-based matrix
//			my_current_offset += sizeof(int);
//			MPI_File_read_at(fh_RD_kernel, my_current_offset, &colIdx, 1,
//					MPI_INT, &status);
//			colIdx--;//0-based matrix
//			my_current_offset += sizeof(int);
//			MPI_File_read_at(fh_RD_kernel, my_current_offset, &val, 1,
//					MPI_DOUBLE, &status);
//			my_current_offset += sizeof(double);
//			spMatCSC->row_idx[totalIdxOfCSC]=rowIdx;
//
//			spMatCSC->val[totalIdxOfCSC]=val;
//
//			totalIdxOfCSC++;
//		}
//	}
//
//
//
//
//
//#ifdef DEBUG
//	MPI_Offset total_number_of_bytes;
//	MPI_File_get_size(fh_RD_kernel, &total_number_of_bytes);
//	printf("%d: total_number_of_bytes=%d\n", procRank,(int) total_number_of_bytes);
//
//#endif
//
//
///*
//#ifdef DEBUG
//	fprintf(fp_debug, "print CSC matrix column ptr\n");
//	for (i = 0; i <= spMatCSC->numCol; i++) {
//		fprintf(fp_debug, "col_ptr[%d]=%d\t",i, spMatCSC->col_ptr[i]);
//	}
//	fprintf(fp_debug, "\n");
//	for (i = 0; i < spMatCSC->numCol; i++) {
//		int j;
//		int start=spMatCSC->col_ptr[i];
//		int end=spMatCSC->col_ptr[i+1];
//		for (j=start; j<end; j++){
//			fprintf(fp_debug, "[%d][%d] = %e \n", spMatCSC->row_idx[j], i, spMatCSC->val[j]);
//		}
//	}
//	fflush(NULL);//flush all output files
//#endif
//*/
//#ifdef DEBUG
//	fclose(fp_debug);
//#endif
//	MPI_File_close(&fh_RD_kernel);
//	return;
//}





/*
 *	load binary column based file into Basic sparse matrix format
 *
 *	input file kernel_filename:
 (int, row index) + (int, column index)+ (double, double value)
 4 8 7.708820e-01
 5 8 9.082630e-01
 3 10 2.271540e-01
 7 25 6.604270e-01
 1 26 6.365470e-01
 *
 * notice: these below part is not efficient, solution: wrap the data into a new MPI_Datatype,
 * and read a bunch of every time
 MPI_File_read_at_all(fh_RD, my_begin_offset, &rowIdx, 1, MPI_INT, &status);
 my_begin_offset+=sizeof(int);
 MPI_File_read_at_all(fh_RD, my_begin_offset, &colIdx, 1, MPI_INT, &status);
 my_begin_offset+=sizeof(int);
 MPI_File_read_at_all(fh_RD, my_begin_offset, &val, 1, MPI_DOUBLE, &status);
 my_begin_offset+=sizeof(double);
 * */
void mpiio_load_kernelColBin2BasicSpMat(int procRank, int numProc,	char * kernel_filename, int numCol,
		KernelColInfoPerCol * kernelColInfo, int startCol, int endCol, BasicSpMat * basicSpMat)
{
	MPI_Status status;
	/*MPI IO operation*/
	MPI_Offset my_current_offset, dataUnitLenByByte = sizeof(int) + sizeof(int)
			+ sizeof(double);
	MPI_File fh_RD_kernel_info, fh_RD_kernel;
	//   MPI_Status status;
	int file_open_error;

	int rowIdx, colIdx;
	double val;
	int i;

#ifdef DEBUG
	char filename2[FILENAMELEN];
	sprintf(filename2, "rank_%d.mpiioRead", procRank);
	FILE* fp_debug = fopen(filename2, "w");
//	printf("%d: load kernel_info_filename\n", procRank);
#endif


	//read kernel file
	file_open_error = MPI_File_open(MPI_COMM_WORLD, kernel_filename,
			MPI_MODE_RDONLY, MPI_INFO_NULL, &fh_RD_kernel);
	if (file_open_error != MPI_SUCCESS) {
		char error_string[BUFSIZ];		int length_of_error_string, error_class;		MPI_Error_class(file_open_error, &error_class);		MPI_Error_string(error_class, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Error_string(file_open_error, error_string, &length_of_error_string);		printf("%3d: %s\n", procRank, error_string);		MPI_Abort(MPI_COMM_WORLD, file_open_error);
	}

#ifdef DEBUG
	MPI_Offset total_number_of_bytes;
	MPI_File_get_size(fh_RD_kernel, &total_number_of_bytes);
	printf("%d: total_number_of_bytes=%d\n", procRank,(int) total_number_of_bytes);
	fprintf(fp_debug, "%d: startCol=%d, endCol=%d\n", procRank, startCol,endCol);
#endif

	//load binary kernel data ((((
	for (i = startCol; i <= endCol; i++) {
		if (kernelColInfo[i].nnz != 0) {//not every i has value
			int j;
			for (j = 0; j < kernelColInfo[i].nnz; j++) {
				int index = kernelColInfo[i].globalBeginIdx + j;
				my_current_offset = index * (dataUnitLenByByte);

				#ifdef DUBUG //cannot be included in for loop
				fprintf(fp_debug, "%d: index=%d my_current_offset=%d,\n",
						procRank, index, (int) my_current_offset);
				//    			fprintf(stdout, "%d: index=%d my_current_offset=%d,\n", procRank, index,(int)my_current_offset);
				#endif

				MPI_File_read_at(fh_RD_kernel, my_current_offset, &rowIdx, 1,
						MPI_INT, &status);
				rowIdx--;//0-based matrix
				my_current_offset += sizeof(int);
				MPI_File_read_at(fh_RD_kernel, my_current_offset, &colIdx, 1,
						MPI_INT, &status);
				colIdx--;//0-based matrix
				my_current_offset += sizeof(int);
				MPI_File_read_at(fh_RD_kernel, my_current_offset, &val, 1,
						MPI_DOUBLE, &status);
				my_current_offset += sizeof(double);
				basicSpMat->nonzeroEntry[basicSpMat->count].rowIndex=rowIdx;
				basicSpMat->nonzeroEntry[basicSpMat->count].columnIndex=colIdx;
				basicSpMat->nonzeroEntry[basicSpMat->count].value=val;
				basicSpMat->count++;
			}
		}
	}
	//load binary kernel data ))))

/*
#ifdef DEBUG
	fprintf(fp_debug, "print basic sparse matrix\n");
	for (i = 0; i < basicSpMat->numNonzero; i++) {
		MatNonzeroEntry entry = basicSpMat->nonzeroEntry[i];
		fprintf(fp_debug, "[%d][%d] = %e \n", entry.rowIndex, entry.columnIndex,
				entry.value);
	}
	fflush(NULL);
#endif
*/
#ifdef DEBUG
	fclose(fp_debug);
#endif
	MPI_File_close(&fh_RD_kernel);
	return;
}


void mpiio_write_result(const int procRank, const int numProc, char * fname, MY_FLOAT_TYPE * local_vec, const int local_n,
		const int local_start_idx, const int tot_len)
{
	int err;
	MPI_Status status;
    MPI_Offset my_begin_offset;/*, dataUnitLenByByte = sizeof(int) + sizeof(int)+ sizeof(double)*/;
    MPI_File  fp;
    MPI_File_delete(fname, MPI_INFO_NULL);
    int file_open_error = MPI_File_open(MPI_COMM_WORLD, fname, MPI_MODE_WRONLY | MPI_MODE_CREATE, MPI_INFO_NULL, &fp);
    if (file_open_error != MPI_SUCCESS) {
		char error_string[BUFSIZ];
		int length_of_error_string, error_class;
		MPI_Error_class(file_open_error, &error_class);
		MPI_Error_string(error_class, error_string, &length_of_error_string);
		printf("%3d: %s\n", procRank, error_string);
		MPI_Error_string(file_open_error, error_string, &length_of_error_string);
		printf("%3d: %s\n", procRank, error_string);
		MPI_Abort(MPI_COMM_WORLD, file_open_error);
    }
    MPI_Offset size = ((size_t)tot_len) * sizeof(MY_FLOAT_TYPE);
    err=MPI_File_preallocate(fp, size);
    assert(err==MPI_SUCCESS);
    my_begin_offset=local_start_idx * sizeof(MY_FLOAT_TYPE);
    MPI_File_write_at(fp, my_begin_offset, local_vec, local_n, MY_MPI_FLOAT_TYPE, &status);
    assert(err==MPI_SUCCESS);
    MPI_File_close(&fp);
    assert(err==MPI_SUCCESS);
}


/*
 void readBin(char * filename)
 {
 //		char filename[1024]="kernel_col.mm2.bin" "199406091622.TATO.BHZ.RAYL.000.aq.bin";
 FILE *fp = fopen(filename, "rb");
 if (fp==NULL) {
 fprintf(stderr, "Cannot open the file: %s  \n", filename);
 exit(EXIT_FAILURE);
 }
 //reading binary files
 int index[2]={0,0};
 double val=0.0;
 while(feof(fp)==0){
 fread(index, sizeof(int),2, fp);
 fread(&val, sizeof(double),1, fp);
 printf("index=%d %d, val=%e \n",index[0], index[1],val );
 }
 fclose(fp);
 }
 */
void initSpMatCSC(SpMatCSC * spMatCSC, int numOfRows, int numOfColumns,long long numOfNonZeroEntries)
{
	spMatCSC->numRow = numOfRows;
	spMatCSC->numCol = numOfColumns;
	spMatCSC->numNonzero = numOfNonZeroEntries;

	spMatCSC->col_ptr = (int*) malloc(sizeof(int) * (spMatCSC->numCol + 1));
	if(NULL==spMatCSC->col_ptr ){
		fprintf(stderr, "spMatCSC->col_ptr calloc fails \n");exit(-1);
	}
	memset(spMatCSC->col_ptr, 0, sizeof(int) * (spMatCSC->numCol + 1));

	spMatCSC->row_idx = (int*) malloc(sizeof(int) * (spMatCSC->numNonzero));
	if(NULL==spMatCSC->row_idx){
		fprintf(stderr, "spMatCSC->row_idx calloc fails \n");exit(-1);
	}
	memset(spMatCSC->row_idx, 0, sizeof(int) * (spMatCSC->numNonzero));

	spMatCSC->val = (MY_FLOAT_TYPE*) malloc(sizeof(MY_FLOAT_TYPE) * (spMatCSC->numNonzero));
	if(NULL==spMatCSC->val){
		fprintf(stderr, "spMatCSC->val calloc fails \n");exit(-1);
	}
	memset(spMatCSC->val, 0, sizeof(MY_FLOAT_TYPE) * (spMatCSC->numNonzero));
}

void finalizeSpMatCSC(SpMatCSC * spMatCSC)
{
	free(spMatCSC->col_ptr);
	free(spMatCSC->row_idx);
	free(spMatCSC->val);
}



void init_BasicSpMat(BasicSpMat *basicSparseMatrix,
		int numOfRows, int numOfColumns, int numOfNonZeroEntries)
{
	basicSparseMatrix->numRow = numOfRows;//1;
	basicSparseMatrix->numCol=numOfColumns;//0;
	basicSparseMatrix->numNonzero = numOfNonZeroEntries;
	basicSparseMatrix->nonzeroEntry	= (MatNonzeroEntry *) malloc(sizeof(MatNonzeroEntry)
					* (basicSparseMatrix->numNonzero));
	if(NULL==basicSparseMatrix->nonzeroEntry){
		fprintf(stderr, "basicSparseMatrix->nonzeroEntry calloc fails \n");exit(-1);
	}
	basicSparseMatrix->count=0;
}
void finalizeBasicSpMat(BasicSpMat *basicSparseMatrix) {
	free(basicSparseMatrix->nonzeroEntry);
}

void init_BasicSpMat2(BasicSpMat2 *basicSparseMatrix,
		int numOfRows, int numOfColumns, int numOfNonZeroEntries)
{
	basicSparseMatrix->numRow = numOfRows;//1;
	basicSparseMatrix->numCol=numOfColumns;//0;
	basicSparseMatrix->numNonzero = numOfNonZeroEntries;
	basicSparseMatrix->rowIdx=(int *)calloc(numOfNonZeroEntries, sizeof(int));
	if(NULL==basicSparseMatrix->rowIdx){
		fprintf(stderr, "basicSparseMatrix->rowIdx calloc fails \n");exit(-1);
	}
	basicSparseMatrix->colIdx=(int *)calloc(numOfNonZeroEntries, sizeof(int));
	if(NULL==basicSparseMatrix->colIdx){
		fprintf(stderr, "basicSparseMatrix->colIdx calloc fails \n");exit(-1);
	}
	basicSparseMatrix->val=(MY_FLOAT_TYPE *) calloc(numOfNonZeroEntries, sizeof(MY_FLOAT_TYPE));
	if(NULL==basicSparseMatrix->val){
		fprintf(stderr, "basicSparseMatrix->val calloc fails \n");exit(-1);
	}
	basicSparseMatrix->count=0;
}
void finalizeBasicSpMat2(BasicSpMat2 *basicSparseMatrix){
	free(basicSparseMatrix->rowIdx);
	free(basicSparseMatrix->colIdx);
	free(basicSparseMatrix->val);
}
/* 3/10/2013
 * read row or column partition files
 *
 * ker_col_64.index: kernel column and vector x partition (the 64 means number of cores)
damp_row_64.index : damping row and vector y partition (the 64 means number of cores)

in "ker_col_64.index" file, the column range of the kernel matrix for each core are stored in each row. For example,
1  12729592                 ==> column range of the kernel matrix (vector x) for the first core
12729593  16414717   ==> column range of the kernel matrix (vector x) for the second core

Note that the value starts from 1.

in "damp_row_64.index" file, the row range of the damping for each core are stored in each row. For example,
1  83530320                  ==> row range of the damping matrix (vector y) for the first core
83530321  109251895  ==> row range of the damping matrix (vector y) for the second core */
void readPartitionFile(const char * fname, const int rank, const int size, unsigned int * start, unsigned int * end){
	unsigned int * start_indices=(unsigned int *)calloc(size, sizeof(*start_indices));
	unsigned int * end_indices=(unsigned int *)calloc(size, sizeof(*end_indices));
	FILE * fp = fopen(fname, "r");
	assert(fp);
	int i;
	for(i=0; i<size; i++){
		fscanf(fp, "%i %i\n", start_indices+i, end_indices+i);
		//adjust from 1 indexing to 0 indexing
		start_indices[i]--;
		end_indices[i]--;
	}
	fclose(fp);
	*start=start_indices[rank];
	*end=end_indices[rank];
	free(start_indices);
	free(end_indices);
}
